{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-17T18:45:35.142506Z",
     "end_time": "2023-04-17T18:45:41.105073Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_seq_items = 100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, KMeansSMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "from ADVO.generator import Generator\n",
    "from ADVO.oversampler import ADVO, TimeGANOverSampler, CTGANOverSampler\n",
    "from ADVO.utils import evaluate_models, compute_kde_difference_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "File `'../worldline_home/shared_functions_basic.ipynb'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[1;32mc:\\users\\dalun\\pycharmprojects\\adv-o\\venv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:701\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[1;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[0;32m    700\u001B[0m     fpath \u001B[38;5;241m=\u001B[39m arg_lst[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m--> 701\u001B[0m     filename \u001B[38;5;241m=\u001B[39m \u001B[43mfile_finder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mc:\\users\\dalun\\pycharmprojects\\adv-o\\venv\\lib\\site-packages\\IPython\\utils\\path.py:90\u001B[0m, in \u001B[0;36mget_py_filename\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m     89\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m py_name\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFile `\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m` not found.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m name)\n",
      "\u001B[1;31mOSError\u001B[0m: File `'../worldline_home/shared_functions_basic.ipynb'` not found.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m#!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/shared_functions_basic.ipynb\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrun\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../worldline_home/shared_functions_basic.ipynb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m#%run ../../../worldline_home/worldline_home/shared_functions_basic.ipynb\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\dalun\\pycharmprojects\\adv-o\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2414\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[1;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[0;32m   2412\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[0;32m   2413\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m-> 2414\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2416\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[0;32m   2417\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[0;32m   2418\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[0;32m   2419\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32mc:\\users\\dalun\\pycharmprojects\\adv-o\\venv\\lib\\site-packages\\IPython\\core\\magics\\execution.py:712\u001B[0m, in \u001B[0;36mExecutionMagics.run\u001B[1;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[0;32m    710\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnt\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m re\u001B[38;5;241m.\u001B[39mmatch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m^\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.*\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m$\u001B[39m\u001B[38;5;124m\"\u001B[39m,fpath):\n\u001B[0;32m    711\u001B[0m         warn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFor Windows, use double quotes to wrap a filename: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124mun \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmypath\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mmyfile.py\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 712\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    714\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fpath \u001B[38;5;129;01min\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mmeta_path:\n",
      "\u001B[1;31mException\u001B[0m: File `'../worldline_home/shared_functions_basic.ipynb'` not found."
     ]
    }
   ],
   "source": [
    "# Necessary imports for this notebook\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# For Pandas parallelisation\n",
    "from pandarallel import pandarallel\n",
    "# pandarallel.initialize(nb_workers=20)\n",
    "pandarallel.initialize( use_memory_fs=False, nb_workers=10) # \n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/shared_functions_basic.ipynb\n",
    "%run ../worldline_home/shared_functions_basic.ipynb\n",
    "#%run ../../../worldline_home/worldline_home/shared_functions_basic.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['TERM_COUNTRY', 'TERM_MCC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of pickle files, put them together in a single dataframe, and order them by time\n",
    "# It takes as input the folder DIR_INPUT where the files are stored, and the BEGIN_DATE and END_DATE\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    \n",
    "    files = [join(DIR_INPUT, f) for f in listdir(DIR_INPUT) if f>=BEGIN_DATE+'.pkl' and f<=END_DATE+'.pkl']\n",
    "\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final=df_final.sort_values('TX_ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    #df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def clean_categorical(transactions_df):\n",
    "    \n",
    "    all_features = transactions_df.columns\n",
    "\n",
    "    prel_df = transactions_df[transactions_df['TX_FRAUD']==1]\n",
    "    #print(prel_df.loc[prel_df['TERM_COUNTRY']=='SLV'])\n",
    "    prel_df_gen = transactions_df[transactions_df['TX_FRAUD']==0] \n",
    "    categorical = ['TERM_COUNTRY', 'TERM_MCC']\n",
    "    \n",
    "    #print('Expected number is ' + str(prel_df['TERM_COUNTRY'].value_counts()['SLV']))\n",
    "    \n",
    "    features_counts = {}\n",
    "    features_counts_gen = {}\n",
    "    \n",
    "    for column in categorical:\n",
    "        print(column)\n",
    "        features_counts[column] =  prel_df[column].value_counts()\n",
    "        features_counts_gen[column] =  prel_df_gen[column].value_counts()\n",
    "    #print(features_counts['TERM_COUNTRY']['SLV'])\n",
    "    values_to_keep = {}\n",
    "    percentages = {}\n",
    "\n",
    "    counter = 0 \n",
    "    sum_freq = 0\n",
    "\n",
    "    #print('Feature counts terminals columns are ' + str(features_counts['TERM_COUNTRY'].keys()))\n",
    "    #print('Single features are:')\n",
    "    for feature in features_counts.keys():\n",
    "        values_to_keep[feature] = []\n",
    "        percentages[feature] = {}\n",
    "        frequencies = 0\n",
    "        #print(features_counts[feature].keys())\n",
    "        for subkey in features_counts[feature].keys():\n",
    "            # print('Subkey is '+str(subkey) + ', values are ' + str(features_counts_gen[feature][subkey]) )\n",
    "            try:\n",
    "                frequency = features_counts[feature][subkey]/prel_df_gen.shape[0]\n",
    "            except:\n",
    "                print('We are looking into ' + str(features_counts[feature]) + ', for subkey ' + str(subkey) )\n",
    "                \n",
    "                frequency = features_counts[feature][subkey]/prel_df_gen.shape[0]\n",
    "            \n",
    "            values_to_keep[feature].append(subkey)\n",
    "            if subkey in   features_counts_gen[feature].keys():\n",
    "                percentages[feature][subkey] =    features_counts[feature][subkey] / (features_counts[feature][subkey] + features_counts_gen[feature][subkey])\n",
    "            else: \n",
    "                percentages[feature][subkey] =    1\n",
    "            #else:\n",
    "            #    sum_freq +=  (features_counts[feature][subkey] /  features_counts_gen[feature][subkey]) * frequency\n",
    "            #    frequencies += frequency\n",
    "            #    counter = counter + 1\n",
    "        # if counter >0:\n",
    "        for subkey in features_counts_gen[feature].keys():\n",
    "            if subkey not in percentages[feature].keys():\n",
    "                percentages[feature][subkey] = 0\n",
    "    #print(percentages)\n",
    "    #Scale percentages                \n",
    "    for key in percentages.keys():\n",
    "        max_val = max(percentages[key].values())\n",
    "        percentages[key] = {k: v/max_val for k, v in percentages[key].items()}\n",
    "        \n",
    "    for column in categorical:\n",
    "        transactions_df.replace(percentages[column], inplace=True)    \n",
    "    return transactions_df\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_and_clean_realData(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    df = read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "    df.rename({'TX_TIME_DAYS': 'TX_DAY', 'TX_TIME_SECONDS': 'TX_TIME'}, inplace=True, axis = 1)\n",
    "    df.drop(['TX_3D_SECURE', 'TX_LOCAL_AMOUNT',\n",
    "        'TX_LOCAL_CURRENCY', 'TX_CARD_ENTRY_MODE', 'CARD_AUTHENTICATION','TX_INTL', 'AGE', 'LANGUAGE', 'GENDER', 'BROKER', 'ZIP',\n",
    "        'INS_CODE', 'CITY', 'COUNTRY', 'PROVINCE_CODE', 'DISTRICT_CODE',\n",
    "        'CARD_BRAND', 'CARD_EXPIRY', 'CARD_TYPE', 'CREDIT_LIMIT', 'TX_ECOM_IND', 'TX_ID'], axis=1, inplace=True)\n",
    "    df = clean_categorical(df)\n",
    "    df.rename({'TERM_MCC': 'X_TERMINAL', 'TERM_COUNTRY': 'Y_TERMINAL'}, inplace=True, axis = 1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_INPUT ='../worldline_home/2018/baseline/data_clean/'\n",
    "#DIR_INPUT='../worldline_home/media/hdd3/worldline_home/2018/baseline/data_clean/' \n",
    "#DIR_INPUT = '2018/baseline/data_clean/'\n",
    "\n",
    "BEGIN_DATE = \"2018-04-01\"\n",
    "END_DATE = \"2018-04-20\"\n",
    "# To load everything:\n",
    "# END_DATE = \"2018-09-30\"\n",
    "\n",
    "#BEGIN_DATE = \"2018-07-25\"\n",
    "#END_DATE = \"2018-08-31\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=retrieve_and_clean_realData(DIR_INPUT, BEGIN_DATE, END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_STRATEGY = 0.18\n",
    "N_JOBS = 10\n",
    "N_TREES = 20\n",
    "N_USERS = 10000\n",
    "N_TERMINALS = 1000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "RANDOM_GRID_RF = {'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_features': [1, 'sqrt', 'log2'], 'max_depth': [5, 16, 28, 40, None], 'min_samples_split': [10, 25, 50], 'min_samples_leaf': [4, 8, 32], 'bootstrap': [True, False]}\n",
    "RANDOM_GRID_RIDGE = {'alpha': [int(x) for x in np.linspace(start = 0.001, stop = 1, num = 100)], 'fit_intercept': [True, False]}\n",
    "RANDOM_GRID_NN = {'hidden_layer_sizes': [int(x) for x in np.linspace(start = 1, stop = 41, num = 80)], 'alpha': [int(x) for x in np.linspace(start = 0.005, stop = 0.02, num = 100)]}\n",
    "\n",
    "\n",
    "CANDIDATE_REGRESSORS = [MLPRegressor(max_iter=2000, random_state=RANDOM_STATE), Ridge(random_state=RANDOM_STATE), RandomForestRegressor(random_state=RANDOM_STATE)]\n",
    "CANDIDATE_GRIDS = [RANDOM_GRID_NN, RANDOM_GRID_RIDGE, RANDOM_GRID_RF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(X_train,y_train,learner, X_test, predictions_proba, discrete_predictions):\n",
    "    learner.fit(X_train, y_train)\n",
    "    y_hat = learner.predict(X_test)\n",
    "    y_hat_proba = learner.predict_proba(X_test)[:,1]\n",
    "    predictions_proba.append(y_hat_proba)\n",
    "    discrete_predictions.append(y_hat)\n",
    "\n",
    "def run_advo(X_train, y_train, window_counter):\n",
    "    advo = ADVO(n_jobs=N_JOBS,sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE, mimo=False)\n",
    "    advo.set_transactions(X_train, y_train)\n",
    "    advo.create_couples()\n",
    "    regressor_scores = advo.select_best_regressor(candidate_regressors=CANDIDATE_REGRESSORS,parameters_set=CANDIDATE_GRIDS)\n",
    "    advo.tune_best_regressors()\n",
    "    advo.fit_regressors()\n",
    "    advo.transactions_df = advo.insert_synthetic_frauds(advo.transactions_df)\n",
    "    regressor_scores.to_csv('realresults/regressor_scores_'+str(window_counter)+'.csv', index=False)\n",
    "    return advo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(train_size_days=5, test_size_days=2):\n",
    "\n",
    "    #transactions_df = Generator().generate(filename='dataset_six_months.csv',nb_days_to_generate=180)\n",
    "    #transactions_df = pd.read_csv('utils/dataset_six_months.csv', parse_dates=['TX_DATETIME'])\n",
    "\n",
    "    start_date, end_date = transactions_df['TX_DATETIME'].min(), transactions_df['TX_DATETIME'].max()\n",
    "    \n",
    "    window_start, window_end, window_counter  = start_date, start_date + timedelta(days=train_size_days), 0\n",
    "    while window_end <= end_date:\n",
    "        print('Window: ', window_counter, ' - ', window_start, ' - ', window_end)\n",
    "\n",
    "        # Split data into train and test according to the window\n",
    "        train_mask, test_mask = (transactions_df['TX_DATETIME'] >= window_start) & (transactions_df['TX_DATETIME'] < window_end), (transactions_df['TX_DATETIME'] >= window_end) & (transactions_df['TX_DATETIME'] < window_end + timedelta(days=test_size_days))\n",
    "        X_train, y_train, X_test, y_test = transactions_df[train_mask].drop(columns=['TX_FRAUD']), transactions_df[train_mask]['TX_FRAUD'], transactions_df[test_mask].drop(columns=['TX_FRAUD']), transactions_df[test_mask]['TX_FRAUD']\n",
    "        training_variables, predictions_proba, discrete_predictions = ['X_TERMINAL', 'Y_TERMINAL', 'TX_AMOUNT'], [], []\n",
    "\n",
    "        # Oversample data using ADVO, SMOTE, RandomOverSampler and KMeansSMOTE\n",
    "        advo = run_advo(X_train, y_train, window_counter)\n",
    "        kmeans_smote = KMeansSMOTE(n_jobs=N_JOBS, kmeans_estimator=MiniBatchKMeans(n_init=3),sampling_strategy=SAMPLE_STRATEGY, cluster_balance_threshold=0.005, random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        smote = SMOTE(k_neighbors=NearestNeighbors(n_jobs=N_JOBS),sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        random = RandomOverSampler(sampling_strategy=SAMPLE_STRATEGY, random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        timegan = TimeGANOverSampler(sampling_strategy=SAMPLE_STRATEGY, epochs=100, seq_len=4, n_seq=3, hidden_dim=24, gamma=1, noise_dim = 32, dim = 128, batch_size = 32, log_step = 100, learning_rate = 5e-4,random_state=RANDOM_STATE).fit_resample(X_train[training_variables+['CUSTOMER_ID']], y_train)\n",
    "        ctgan = CTGANOverSampler(sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "    \n",
    "        names = ['Baseline','Baseline_balanced', 'SMOTE','Random', 'KMeansSMOTE', 'CTGAN','TIMEGAN', 'ADVO']\n",
    "        Xy = [(X_train[training_variables], y_train), kmeans_smote, smote, random, ctgan, timegan, (advo.transactions_df[advo.useful_features], advo.transactions_df['TX_FRAUD'])]\n",
    "\n",
    "        fit_predict(X_train[training_variables],y_train, RandomForestClassifier(n_estimators=N_TREES ,n_jobs=N_JOBS, random_state=RANDOM_STATE) , X_test[training_variables], predictions_proba, discrete_predictions)\n",
    "        for X, y in Xy:\n",
    "            fit_predict(X,y, BalancedRandomForestClassifier(n_estimators=N_TREES ,n_jobs=N_JOBS, random_state=RANDOM_STATE) , X_test[training_variables], predictions_proba, discrete_predictions)\n",
    "\n",
    "        # Compute metrics\n",
    "        _, all_metrics = evaluate_models(predictions_proba, discrete_predictions, X_test['CUSTOMER_ID'], names, y_test, K_needed = [50, 100, 200, 500, 1000, 2000])\n",
    "        all_metrics.to_csv('realresults/all_metrics_'+str(window_counter)+'.csv', index=False)\n",
    "        trapzs = compute_kde_difference_auc(Xy, training_variables, names)\n",
    "        trapzs.to_csv('realresults/trapz_'+str(window_counter)+'.csv', index=False)\n",
    "        \n",
    "\n",
    "        window_start, window_end, window_counter  = window_end, window_end + timedelta(days=train_size_days), window_counter + 1\n",
    "        print('Window ', window_counter, ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window:  0  -  2018-04-01 01:00:02  -  2018-04-06 01:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|████████████████████| 100/100 [00:18<00:00,  5.43it/s]\n",
      "Supervised network training: 100%|███████████████████| 100/100 [00:13<00:00,  7.68it/s]\n",
      "Joint networks training: 100%|███████████████████████| 100/100 [03:13<00:00,  1.93s/it]\n",
      "Synthetic data generation: 100%|███████████████████| 2902/2902 [13:30<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window  1  done\n",
      "Window:  1  -  2018-04-06 01:00:02  -  2018-04-11 01:00:02\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No clusters found with sufficient samples of class 1. Try lowering the cluster_balance_threshold or increasing the number of clusters.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(RANDOM_STATE)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mmake_classification\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_size_days\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size_days\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[28], line 19\u001B[0m, in \u001B[0;36mmake_classification\u001B[0;34m(train_size_days, test_size_days)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Oversample data using ADVO, SMOTE, RandomOverSampler and KMeansSMOTE\u001B[39;00m\n\u001B[1;32m     18\u001B[0m advo \u001B[38;5;241m=\u001B[39m run_advo(X_train, y_train, window_counter)\n\u001B[0;32m---> 19\u001B[0m kmeans_smote \u001B[38;5;241m=\u001B[39m \u001B[43mKMeansSMOTE\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mN_JOBS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkmeans_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMiniBatchKMeans\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43msampling_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSAMPLE_STRATEGY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcluster_balance_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.005\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRANDOM_STATE\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtraining_variables\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m smote \u001B[38;5;241m=\u001B[39m SMOTE(k_neighbors\u001B[38;5;241m=\u001B[39mNearestNeighbors(n_jobs\u001B[38;5;241m=\u001B[39mN_JOBS),sampling_strategy\u001B[38;5;241m=\u001B[39mSAMPLE_STRATEGY,random_state\u001B[38;5;241m=\u001B[39mRANDOM_STATE)\u001B[38;5;241m.\u001B[39mfit_resample(X_train[training_variables], y_train)\n\u001B[1;32m     21\u001B[0m random \u001B[38;5;241m=\u001B[39m RandomOverSampler(sampling_strategy\u001B[38;5;241m=\u001B[39mSAMPLE_STRATEGY, random_state\u001B[38;5;241m=\u001B[39mRANDOM_STATE)\u001B[38;5;241m.\u001B[39mfit_resample(X_train[training_variables], y_train)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/site-packages/imblearn/base.py:203\u001B[0m, in \u001B[0;36mBaseSampler.fit_resample\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    182\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resample the dataset.\u001B[39;00m\n\u001B[1;32m    183\u001B[0m \n\u001B[1;32m    184\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03m    The corresponding label of `X_resampled`.\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m--> 203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/site-packages/imblearn/base.py:88\u001B[0m, in \u001B[0;36mSamplerMixin.fit_resample\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m     82\u001B[0m X, y, binarize_y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_X_y(X, y)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampling_strategy_ \u001B[38;5;241m=\u001B[39m check_sampling_strategy(\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msampling_strategy, y, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampling_type\n\u001B[1;32m     86\u001B[0m )\n\u001B[0;32m---> 88\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_resample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m y_ \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     91\u001B[0m     label_binarize(output[\u001B[38;5;241m1\u001B[39m], classes\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39munique(y)) \u001B[38;5;28;01mif\u001B[39;00m binarize_y \u001B[38;5;28;01melse\u001B[39;00m output[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     92\u001B[0m )\n\u001B[1;32m     94\u001B[0m X_, y_ \u001B[38;5;241m=\u001B[39m arrays_transformer\u001B[38;5;241m.\u001B[39mtransform(output[\u001B[38;5;241m0\u001B[39m], y_)\n",
      "File \u001B[0;32m/usr/local/lib/python3.8/site-packages/imblearn/over_sampling/_smote/cluster.py:265\u001B[0m, in \u001B[0;36mKMeansSMOTE._fit_resample\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    262\u001B[0m cluster_weights \u001B[38;5;241m=\u001B[39m cluster_sparsities \u001B[38;5;241m/\u001B[39m cluster_sparsities\u001B[38;5;241m.\u001B[39msum()\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m valid_clusters:\n\u001B[0;32m--> 265\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    266\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo clusters found with sufficient samples of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    267\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclass \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_sample\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Try lowering the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    268\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcluster_balance_threshold or increasing the number of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    269\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclusters.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    270\u001B[0m     )\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m valid_cluster_idx, valid_cluster \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(valid_clusters):\n\u001B[1;32m    273\u001B[0m     X_cluster \u001B[38;5;241m=\u001B[39m _safe_indexing(X, valid_cluster)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: No clusters found with sufficient samples of class 1. Try lowering the cluster_balance_threshold or increasing the number of clusters."
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "make_classification(train_size_days=5, test_size_days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_transactions_df = pd.read_csv('utils/dataset_six_months.csv', parse_dates=['TX_DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_transactions_df.columns, transactions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column for column in synthetic_transactions_df.columns if column not in transactions_df.columns], [column for column in transactions_df.columns if column not in synthetic_transactions_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tr=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraufs_df = tr[tr['TX_FRAUD'] == 1]\n",
    "fraufs_df[fraufs_df['TERM_COUNTRY']=='SLV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraufs_df = tr[tr['TX_FRAUD'] == 1]['TERM_COUNTRY'].value_counts()['SLV']\n",
    "fraufs_df#[fraufs_df['TERM_MCC']=='9999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7929  VS 9999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
