{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_seq_items = 100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, KMeansSMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "from ADVO.generator import Generator\n",
    "from ADVO.oversampler import ADVO, TimeGANOverSampler, CTGANOverSampler\n",
    "from ADVO.utils import evaluate_models, compute_kde_difference_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports for this notebook\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# For Pandas parallelisation\n",
    "from pandarallel import pandarallel\n",
    "# pandarallel.initialize(nb_workers=20)\n",
    "pandarallel.initialize( use_memory_fs=False, nb_workers=10) # \n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/shared_functions_basic.ipynb\n",
    "%run ../worldline_home/shared_functions_basic.ipynb\n",
    "#%run ../../../worldline_home/worldline_home/shared_functions_basic.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['TERM_COUNTRY', 'TERM_MCC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of pickle files, put them together in a single dataframe, and order them by time\n",
    "# It takes as input the folder DIR_INPUT where the files are stored, and the BEGIN_DATE and END_DATE\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    \n",
    "    files = [join(DIR_INPUT, f) for f in listdir(DIR_INPUT) if f>=BEGIN_DATE+'.pkl' and f<=END_DATE+'.pkl']\n",
    "\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final=df_final.sort_values('TX_ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    #df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def clean_categorical(transactions_df):\n",
    "    \n",
    "    all_features = transactions_df.columns\n",
    "\n",
    "    prel_df = transactions_df[transactions_df['TX_FRAUD']==1]\n",
    "    #print(prel_df.loc[prel_df['TERM_COUNTRY']=='SLV'])\n",
    "    prel_df_gen = transactions_df[transactions_df['TX_FRAUD']==0] \n",
    "    categorical = ['TERM_COUNTRY', 'TERM_MCC']\n",
    "    \n",
    "    #print('Expected number is ' + str(prel_df['TERM_COUNTRY'].value_counts()['SLV']))\n",
    "    \n",
    "    features_counts = {}\n",
    "    features_counts_gen = {}\n",
    "    \n",
    "    for column in categorical:\n",
    "        print(column)\n",
    "        features_counts[column] =  prel_df[column].value_counts()\n",
    "        features_counts_gen[column] =  prel_df_gen[column].value_counts()\n",
    "    #print(features_counts['TERM_COUNTRY']['SLV'])\n",
    "    values_to_keep = {}\n",
    "    percentages = {}\n",
    "\n",
    "    counter = 0 \n",
    "    sum_freq = 0\n",
    "\n",
    "    #print('Feature counts terminals columns are ' + str(features_counts['TERM_COUNTRY'].keys()))\n",
    "    #print('Single features are:')\n",
    "    for feature in features_counts.keys():\n",
    "        values_to_keep[feature] = []\n",
    "        percentages[feature] = {}\n",
    "        frequencies = 0\n",
    "        #print(features_counts[feature].keys())\n",
    "        for subkey in features_counts[feature].keys():\n",
    "            # print('Subkey is '+str(subkey) + ', values are ' + str(features_counts_gen[feature][subkey]) )\n",
    "            try:\n",
    "                frequency = features_counts[feature][subkey]/prel_df_gen.shape[0]\n",
    "            except:\n",
    "                print('We are looking into ' + str(features_counts[feature]) + ', for subkey ' + str(subkey) )\n",
    "                \n",
    "                frequency = features_counts[feature][subkey]/prel_df_gen.shape[0]\n",
    "            \n",
    "            values_to_keep[feature].append(subkey)\n",
    "            if subkey in   features_counts_gen[feature].keys():\n",
    "                percentages[feature][subkey] =    features_counts[feature][subkey] / (features_counts[feature][subkey] + features_counts_gen[feature][subkey])\n",
    "            else: \n",
    "                percentages[feature][subkey] =    1\n",
    "            #else:\n",
    "            #    sum_freq +=  (features_counts[feature][subkey] /  features_counts_gen[feature][subkey]) * frequency\n",
    "            #    frequencies += frequency\n",
    "            #    counter = counter + 1\n",
    "        # if counter >0:\n",
    "        for subkey in features_counts_gen[feature].keys():\n",
    "            if subkey not in percentages[feature].keys():\n",
    "                percentages[feature][subkey] = 0\n",
    "    #print(percentages)\n",
    "    #Scale percentages                \n",
    "    for key in percentages.keys():\n",
    "        max_val = max(percentages[key].values())\n",
    "        percentages[key] = {k: v/max_val for k, v in percentages[key].items()}\n",
    "        \n",
    "    for column in categorical:\n",
    "        transactions_df.replace(percentages[column], inplace=True)    \n",
    "    return transactions_df\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_and_clean_realData(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    df = read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "    df.rename({'TX_TIME_DAYS': 'TX_DAY', 'TX_TIME_SECONDS': 'TX_TIME'}, inplace=True, axis = 1)\n",
    "    df.drop(['TX_3D_SECURE', 'TX_LOCAL_AMOUNT',\n",
    "        'TX_LOCAL_CURRENCY', 'TX_CARD_ENTRY_MODE', 'CARD_AUTHENTICATION','TX_INTL', 'AGE', 'LANGUAGE', 'GENDER', 'BROKER', 'ZIP',\n",
    "        'INS_CODE', 'CITY', 'COUNTRY', 'PROVINCE_CODE', 'DISTRICT_CODE',\n",
    "        'CARD_BRAND', 'CARD_EXPIRY', 'CARD_TYPE', 'CREDIT_LIMIT', 'TX_ECOM_IND', 'TX_ID'], axis=1, inplace=True)\n",
    "    df = clean_categorical(df)\n",
    "    df.rename({'TERM_MCC': 'X_TERMINAL', 'TERM_COUNTRY': 'Y_TERMINAL'}, inplace=True, axis = 1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%#time` not found.\n"
     ]
    }
   ],
   "source": [
    "DIR_INPUT ='../worldline_home/2018/baseline/data_clean/'\n",
    "#DIR_INPUT='../worldline_home/media/hdd3/worldline_home/2018/baseline/data_clean/' \n",
    "#DIR_INPUT = '2018/baseline/data_clean/'\n",
    "\n",
    "BEGIN_DATE = \"2018-04-01\"\n",
    "END_DATE = \"2018-09-30\"\n",
    "# To load everything:\n",
    "# END_DATE = \"2018-09-30\"\n",
    "\n",
    "#BEGIN_DATE = \"2018-07-25\"\n",
    "#END_DATE = \"2018-08-31\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "#%time transactions_df=retrieve_and_clean_realData(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "#transactions_df.to_csv('CategoricallyEncodedData.csv')\n",
    "\n",
    "%#time transactions_df = pd.read_csv('CategoricallyEncodedData.csv').iloc[:,1:]\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions_df =transactions_df.iloc[:,1:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_STRATEGY = 0.18\n",
    "N_JOBS = 10\n",
    "N_TREES = 20\n",
    "N_USERS = 10000\n",
    "N_TERMINALS = 1000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "RANDOM_GRID_RF = {'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_features': [1, 'sqrt', 'log2'], 'max_depth': [5, 16, 28, 40, None], 'min_samples_split': [10, 25, 50], 'min_samples_leaf': [4, 8, 32], 'bootstrap': [True, False]}\n",
    "RANDOM_GRID_RIDGE = {'alpha': [int(x) for x in np.linspace(start = 0.001, stop = 1, num = 100)], 'fit_intercept': [True, False]}\n",
    "RANDOM_GRID_NN = {'hidden_layer_sizes': [int(x) for x in np.linspace(start = 1, stop = 41, num = 80)], 'alpha': [int(x) for x in np.linspace(start = 0.005, stop = 0.02, num = 100)]}\n",
    "\n",
    "#CANDIDATE_REGRESSORS = [MLPRegressor(max_iter=2000, random_state=RANDOM_STATE), Ridge(random_state=RANDOM_STATE), RandomForestRegressor(random_state=RANDOM_STATE)]\n",
    "CANDIDATE_REGRESSORS = [Ridge(random_state=RANDOM_STATE), RandomForestRegressor(random_state=RANDOM_STATE)]\n",
    "#CANDIDATE_GRIDS = [RANDOM_GRID_NN, RANDOM_GRID_RIDGE, RANDOM_GRID_RF]\n",
    "CANDIDATE_GRIDS = [ RANDOM_GRID_RIDGE, RANDOM_GRID_RF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(X_train,y_train,learner, X_test, predictions_proba, discrete_predictions):\n",
    "    learner.fit(X_train, y_train)\n",
    "    y_hat = learner.predict(X_test)\n",
    "    y_hat_proba = learner.predict_proba(X_test)[:,1]\n",
    "    predictions_proba.append(y_hat_proba)\n",
    "    discrete_predictions.append(y_hat)\n",
    "\n",
    "def run_advo(X_train, y_train, window_counter):\n",
    "    advo = ADVO(n_jobs=N_JOBS,sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE, mimo=False)\n",
    "    advo.set_transactions(X_train, y_train)\n",
    "    advo.create_couples()\n",
    "    regressor_scores = advo.select_best_regressor(candidate_regressors=CANDIDATE_REGRESSORS,parameters_set=CANDIDATE_GRIDS)\n",
    "    advo.tune_best_regressors()\n",
    "    advo.fit_regressors()\n",
    "    advo.transactions_df = advo.insert_synthetic_frauds(advo.transactions_df)\n",
    "    regressor_scores.to_csv('results_synthetic/regressor_scores_'+str(window_counter)+'.csv', index=False)\n",
    "    return advo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def make_classification(train_size_days=10, test_size_days=10):\n",
    "\n",
    "    transactions_df = Generator(radius=1).generate(filename='dataset_six_months2.csv', nb_days_to_generate=15, n_terminals = 1000, n_customers=1000 )\n",
    "    #transactions_df = pd.read_csv('utils/dataset_six_months.csv', parse_dates=['TX_DATETIME'])\n",
    "\n",
    "    start_date, end_date = transactions_df['TX_DATETIME'].min(), transactions_df['TX_DATETIME'].max()\n",
    "    #start_date_strp = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    #end_date_strp = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    window_start, window_end, window_counter  = start_date, start_date + timedelta(days=train_size_days), 0\n",
    "    #window_start = str(window_start_strp)\n",
    "    #window_end = str(window_end_strp)\n",
    "    while window_end <= end_date:\n",
    "        print('Window: ', window_counter, ' - ', window_start, ' - ', window_end)\n",
    "\n",
    "        # Split data into train and test according to the window\n",
    "        train_mask, test_mask = (transactions_df['TX_DATETIME'] >= window_start) & (transactions_df['TX_DATETIME'] < window_end), (transactions_df['TX_DATETIME'] >= window_end) & (transactions_df['TX_DATETIME'] < str(window_end + timedelta(days=test_size_days)))\n",
    "        \n",
    "        # Drop from test_mask the transactions that have the same cardholder as the frauds in train_mask\n",
    "        training_data, test_data = transactions_df[train_mask], transactions_df[test_mask]\n",
    "        training_frauds = training_data[training_data['TX_FRAUD']==1]\n",
    "        test_data = test_data[~test_data['CUSTOMER_ID'].isin(training_frauds['CUSTOMER_ID'].unique())]\n",
    "\n",
    "        #Create final training and test sets\n",
    "        X_train, y_train, X_test, y_test = training_data.drop(columns=['TX_FRAUD']), training_data['TX_FRAUD'], test_data.drop(columns=['TX_FRAUD']), test_data['TX_FRAUD']\n",
    "        training_variables, predictions_proba, discrete_predictions = ['X_TERMINAL', 'Y_TERMINAL', 'TX_AMOUNT'], [], []\n",
    "\n",
    "        # Oversample data using ADVO, SMOTE, RandomOverSampler and KMeansSMOTE\n",
    "        advo = run_advo(X_train, y_train, window_counter)\n",
    "        kmeans_smote = KMeansSMOTE(n_jobs=N_JOBS, kmeans_estimator=MiniBatchKMeans(n_init=3),sampling_strategy=SAMPLE_STRATEGY, cluster_balance_threshold=0.0025, random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        smote = SMOTE(k_neighbors=NearestNeighbors(n_jobs=N_JOBS),sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        random = RandomOverSampler(sampling_strategy=SAMPLE_STRATEGY, random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        timegan = TimeGANOverSampler(sampling_strategy=SAMPLE_STRATEGY, epochs=100, seq_len=4, n_seq=3, hidden_dim=24, gamma=1, noise_dim = 32, dim = 128, batch_size = 32, log_step = 100, learning_rate = 5e-4,random_state=RANDOM_STATE).fit_resample(X_train[training_variables+['CUSTOMER_ID']], y_train)\n",
    "        ctgan = CTGANOverSampler(sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "    \n",
    "        names = ['Baseline','Baseline_balanced', 'SMOTE','Random', 'KMeansSMOTE', 'CTGAN','TIMEGAN', 'ADVO']\n",
    "        Xy = [(X_train[training_variables], y_train), kmeans_smote, smote, random, ctgan, timegan, (advo.transactions_df[advo.useful_features], advo.transactions_df['TX_FRAUD'])]\n",
    "\n",
    "        fit_predict(X_train[training_variables],y_train, RandomForestClassifier(n_estimators=N_TREES ,n_jobs=N_JOBS, random_state=RANDOM_STATE) , X_test[training_variables], predictions_proba, discrete_predictions)\n",
    "        for X, y in Xy:\n",
    "            fit_predict(X,y, BalancedRandomForestClassifier(n_estimators=N_TREES ,n_jobs=N_JOBS, random_state=RANDOM_STATE) , X_test[training_variables], predictions_proba, discrete_predictions)\n",
    "\n",
    "        # Compute metrics\n",
    "        _, all_metrics = evaluate_models(predictions_proba, discrete_predictions, X_test['CUSTOMER_ID'], names, y_test, K_needed = [50, 100, 200, 500, 1000, 2000])\n",
    "        all_metrics.to_csv('results_synthetic/all_metrics_'+str(window_counter)+'.csv', index=False)\n",
    "        trapzs = compute_kde_difference_auc(Xy, training_variables, names)\n",
    "        trapzs.to_csv('results_synthetic/trapz_'+str(window_counter)+'.csv', index=False)\n",
    "        \n",
    "\n",
    "        window_start, window_end, window_counter  = window_end, window_end + timedelta(days=train_size_days), window_counter + 1\n",
    "        #window_end = str(window_end_strp)\n",
    "        #window_start = str(window_start_strp)\n",
    "        print('Window ', window_counter, ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window:  0  -  2018-04-01 00:00:00  -  2018-04-04 00:00:00eneratedTerminal 892 generatedTerminal 894 generatedCustomer 56 transactions generatedCustomer 57 transactions generatedCustomer 65 transactions generatedCustomer 67 transactions generatedCustomer 74 transactions generatedCustomer 73 transactions generatedCustomer 89 transactions generatedCustomer 90 transactions generatedCustomer 326 transactions generatedCustomer 335 transactions generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Emddeding network training:   0%|                                                               | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (32, 4, 3) for input KerasTensor(type_spec=TensorSpec(shape=(32, 4, 3), dtype=tf.float32, name='RealData'), name='RealData', description=\"created by layer 'RealData'\"), but it was called on an input with incompatible shape (20, 4, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (32, 4, 3) for input KerasTensor(type_spec=TensorSpec(shape=(32, 4, 3), dtype=tf.float32, name='RealData'), name='RealData', description=\"created by layer 'RealData'\"), but it was called on an input with incompatible shape (20, 4, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|█████████████████████████████████████████████████████| 100/100 [00:14<00:00,  6.93it/s]\n",
      "Supervised network training: 100%|████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.51it/s]\n",
      "Joint networks training:   0%|                                                                  | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (32, 4, 3) for input KerasTensor(type_spec=TensorSpec(shape=(32, 4, 3), dtype=tf.float32, name='RealData'), name='RealData', description=\"created by layer 'RealData'\"), but it was called on an input with incompatible shape (20, 4, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (32, 4, 3) for input KerasTensor(type_spec=TensorSpec(shape=(32, 4, 3), dtype=tf.float32, name='RealData'), name='RealData', description=\"created by layer 'RealData'\"), but it was called on an input with incompatible shape (20, 4, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (32, 4, 3) for input KerasTensor(type_spec=TensorSpec(shape=(32, 4, 3), dtype=tf.float32, name='RealData'), name='RealData', description=\"created by layer 'RealData'\"), but it was called on an input with incompatible shape (20, 4, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (32, 4, 3) for input KerasTensor(type_spec=TensorSpec(shape=(32, 4, 3), dtype=tf.float32, name='RealData'), name='RealData', description=\"created by layer 'RealData'\"), but it was called on an input with incompatible shape (20, 4, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Joint networks training: 100%|████████████████████████████████████████████████████████| 100/100 [02:53<00:00,  1.73s/it]\n",
      "Synthetic data generation: 100%|██████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window  1  done\n",
      "Window:  1  -  2018-04-04 00:00:00  -  2018-04-07 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|█████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.82it/s]\n",
      "Supervised network training: 100%|████████████████████████████████████████████████████| 100/100 [00:09<00:00, 10.05it/s]\n",
      "Joint networks training: 100%|████████████████████████████████████████████████████████| 100/100 [02:38<00:00,  1.59s/it]\n",
      "Synthetic data generation: 100%|██████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window  2  done\n",
      "Window:  2  -  2018-04-07 00:00:00  -  2018-04-10 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|█████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.72it/s]\n",
      "Supervised network training: 100%|████████████████████████████████████████████████████| 100/100 [00:19<00:00,  5.19it/s]\n",
      "Joint networks training: 100%|████████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.52s/it]\n",
      "Synthetic data generation: 100%|██████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window  3  done\n",
      "Window:  3  -  2018-04-10 00:00:00  -  2018-04-13 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|█████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.68it/s]\n",
      "Supervised network training: 100%|████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.85it/s]\n",
      "Joint networks training:   4%|██▎                                                       | 4/100 [01:26<19:14, 12.03s/it]"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "make_classification(train_size_days=3, test_size_days=3)\n",
    "print('ewfièufuipedwsrfbjnuewbuiè')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_days=10\n",
    "timedelta(days=train_size_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_transactions_df = pd.read_csv('utils/dataset_six_months.csv', parse_dates=['TX_DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_transactions_df.columns, transactions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column for column in synthetic_transactions_df.columns if column not in transactions_df.columns], [column for column in transactions_df.columns if column not in synthetic_transactions_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tr=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraufs_df = tr[tr['TX_FRAUD'] == 1]\n",
    "fraufs_df[fraufs_df['TERM_COUNTRY']=='SLV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraufs_df = tr[tr['TX_FRAUD'] == 1]['TERM_COUNTRY'].value_counts()['SLV']\n",
    "fraufs_df#[fraufs_df['TERM_MCC']=='9999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7929  VS 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
