{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-16 09:45:48.445082: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-16 09:45:49.639336: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-16 09:45:49.639439: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-16 09:45:49.639453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_seq_items = 100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, KMeansSMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "#from ADVO.generator import Generator\n",
    "from ADVO.oversampler import ADVO, TimeGANOverSampler, CTGANOverSampler\n",
    "from ADVO.utils import evaluate_models, compute_kde_difference_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports for this notebook\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# For Pandas parallelisation\n",
    "from pandarallel import pandarallel\n",
    "# pandarallel.initialize(nb_workers=20)\n",
    "pandarallel.initialize( use_memory_fs=False, nb_workers=10) # \n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/shared_functions_basic.ipynb\n",
    "%run ../worldline_home/shared_functions_basic.ipynb\n",
    "#%run ../../../worldline_home/worldline_home/shared_functions_basic.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['TERM_COUNTRY', 'TERM_MCC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of pickle files, put them together in a single dataframe, and order them by time\n",
    "# It takes as input the folder DIR_INPUT where the files are stored, and the BEGIN_DATE and END_DATE\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    \n",
    "    files = [join(DIR_INPUT, f) for f in listdir(DIR_INPUT) if f>=BEGIN_DATE+'.pkl' and f<=END_DATE+'.pkl']\n",
    "\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final=df_final.sort_values('TX_ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    #df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def clean_categorical(transactions_df):\n",
    "    \n",
    "    all_features = transactions_df.columns\n",
    "\n",
    "    prel_df = transactions_df[transactions_df['TX_FRAUD']==1]\n",
    "    #print(prel_df.loc[prel_df['TERM_COUNTRY']=='SLV'])\n",
    "    prel_df_gen = transactions_df[transactions_df['TX_FRAUD']==0] \n",
    "    categorical = ['TERM_COUNTRY', 'TERM_MCC']\n",
    "    \n",
    "    #print('Expected number is ' + str(prel_df['TERM_COUNTRY'].value_counts()['SLV']))\n",
    "    \n",
    "    features_counts = {}\n",
    "    features_counts_gen = {}\n",
    "    \n",
    "    for column in categorical:\n",
    "        print(column)\n",
    "        features_counts[column] =  prel_df[column].value_counts()\n",
    "        features_counts_gen[column] =  prel_df_gen[column].value_counts()\n",
    "    #print(features_counts['TERM_COUNTRY']['SLV'])\n",
    "    values_to_keep = {}\n",
    "    percentages = {}\n",
    "\n",
    "    counter = 0 \n",
    "    sum_freq = 0\n",
    "\n",
    "    #print('Feature counts terminals columns are ' + str(features_counts['TERM_COUNTRY'].keys()))\n",
    "    #print('Single features are:')\n",
    "    for feature in features_counts.keys():\n",
    "        values_to_keep[feature] = []\n",
    "        percentages[feature] = {}\n",
    "        frequencies = 0\n",
    "        #print(features_counts[feature].keys())\n",
    "        for subkey in features_counts[feature].keys():\n",
    "            # print('Subkey is '+str(subkey) + ', values are ' + str(features_counts_gen[feature][subkey]) )\n",
    "            try:\n",
    "                frequency = features_counts[feature][subkey]/prel_df_gen.shape[0]\n",
    "            except:\n",
    "                print('We are looking into ' + str(features_counts[feature]) + ', for subkey ' + str(subkey) )\n",
    "                \n",
    "                frequency = features_counts[feature][subkey]/prel_df_gen.shape[0]\n",
    "            \n",
    "            values_to_keep[feature].append(subkey)\n",
    "            if subkey in   features_counts_gen[feature].keys():\n",
    "                percentages[feature][subkey] =    features_counts[feature][subkey] / (features_counts[feature][subkey] + features_counts_gen[feature][subkey])\n",
    "            else: \n",
    "                percentages[feature][subkey] =    1\n",
    "            #else:\n",
    "            #    sum_freq +=  (features_counts[feature][subkey] /  features_counts_gen[feature][subkey]) * frequency\n",
    "            #    frequencies += frequency\n",
    "            #    counter = counter + 1\n",
    "        # if counter >0:\n",
    "        for subkey in features_counts_gen[feature].keys():\n",
    "            if subkey not in percentages[feature].keys():\n",
    "                percentages[feature][subkey] = 0\n",
    "    #print(percentages)\n",
    "    #Scale percentages                \n",
    "    for key in percentages.keys():\n",
    "        max_val = max(percentages[key].values())\n",
    "        percentages[key] = {k: v/max_val for k, v in percentages[key].items()}\n",
    "        \n",
    "    for column in categorical:\n",
    "        transactions_df.replace(percentages[column], inplace=True)    \n",
    "    return transactions_df\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_and_clean_realData(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    df = read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "    df.rename({'TX_TIME_DAYS': 'TX_DAY', 'TX_TIME_SECONDS': 'TX_TIME'}, inplace=True, axis = 1)\n",
    "    df.drop(['TX_3D_SECURE', 'TX_LOCAL_AMOUNT',\n",
    "        'TX_LOCAL_CURRENCY', 'TX_CARD_ENTRY_MODE', 'CARD_AUTHENTICATION','TX_INTL', 'AGE', 'LANGUAGE', 'GENDER', 'BROKER', 'ZIP',\n",
    "        'INS_CODE', 'CITY', 'COUNTRY', 'PROVINCE_CODE', 'DISTRICT_CODE',\n",
    "        'CARD_BRAND', 'CARD_EXPIRY', 'CARD_TYPE', 'CREDIT_LIMIT', 'TX_ECOM_IND', 'TX_ID'], axis=1, inplace=True)\n",
    "    df = clean_categorical(df)\n",
    "    df.rename({'TERM_MCC': 'X_TERMINAL', 'TERM_COUNTRY': 'Y_TERMINAL'}, inplace=True, axis = 1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  files\n",
      "CPU times: user 1min 54s, sys: 16.6 s, total: 2min 11s\n",
      "Wall time: 2min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>X_TERMINAL</th>\n",
       "      <th>Y_TERMINAL</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>TX_TIME</th>\n",
       "      <th>TX_DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2236570.0</td>\n",
       "      <td>37__50</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.008694</td>\n",
       "      <td>8.99</td>\n",
       "      <td>2018-09-12 12:41:25</td>\n",
       "      <td>0</td>\n",
       "      <td>14215285.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1485994.0</td>\n",
       "      <td>4833479__17382</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>46.06</td>\n",
       "      <td>2018-09-05 23:34:57</td>\n",
       "      <td>0</td>\n",
       "      <td>13649697.0</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1792717.0</td>\n",
       "      <td>7143782__5</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.008694</td>\n",
       "      <td>1020.00</td>\n",
       "      <td>2018-08-16 19:33:50</td>\n",
       "      <td>0</td>\n",
       "      <td>11907230.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6715254.0</td>\n",
       "      <td>4533745__20247</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>62.99</td>\n",
       "      <td>2018-05-21 20:03:22</td>\n",
       "      <td>0</td>\n",
       "      <td>4392202.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4154156.0</td>\n",
       "      <td>2252003__5</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>53.00</td>\n",
       "      <td>2018-04-19 14:15:54</td>\n",
       "      <td>0</td>\n",
       "      <td>1606554.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_ID     TERMINAL_ID  X_TERMINAL  Y_TERMINAL  TX_AMOUNT  \\\n",
       "0    2236570.0          37__50    0.004588    0.008694       8.99   \n",
       "1    1485994.0  4833479__17382    0.003989    0.008204      46.06   \n",
       "2    1792717.0      7143782__5    0.009363    0.008694    1020.00   \n",
       "3    6715254.0  4533745__20247    0.000085    0.008204      62.99   \n",
       "4    4154156.0      2252003__5    0.002528    0.005959      53.00   \n",
       "\n",
       "           TX_DATETIME  TX_FRAUD     TX_TIME  TX_DAY  \n",
       "0  2018-09-12 12:41:25         0  14215285.0     164  \n",
       "1  2018-09-05 23:34:57         0  13649697.0     157  \n",
       "2  2018-08-16 19:33:50         0  11907230.0     137  \n",
       "3  2018-05-21 20:03:22         0   4392202.0      50  \n",
       "4  2018-04-19 14:15:54         0   1606554.0      18  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR_INPUT ='../worldline_home/2018/baseline/data_clean/'\n",
    "#DIR_INPUT='../worldline_home/media/hdd3/worldline_home/2018/baseline/data_clean/' \n",
    "#DIR_INPUT = '2018/baseline/data_clean/'\n",
    "\n",
    "BEGIN_DATE = \"2018-04-01\"\n",
    "END_DATE = \"2018-09-30\"\n",
    "# To load everything:\n",
    "# END_DATE = \"2018-09-30\"\n",
    "\n",
    "#BEGIN_DATE = \"2018-07-25\"\n",
    "#END_DATE = \"2018-08-31\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "#%time transactions_df=retrieve_and_clean_realData(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "#transactions_df.to_csv('CategoricallyEncodedData.csv')\n",
    "\n",
    "%time transactions_df = pd.read_csv('CategoricallyEncodedData.csv').iloc[:,1:]\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transactions_df =transactions_df.iloc[:,1:].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CUSTOMER_ID</th>\n",
       "      <th>TERMINAL_ID</th>\n",
       "      <th>X_TERMINAL</th>\n",
       "      <th>Y_TERMINAL</th>\n",
       "      <th>TX_AMOUNT</th>\n",
       "      <th>TX_DATETIME</th>\n",
       "      <th>TX_FRAUD</th>\n",
       "      <th>TX_TIME</th>\n",
       "      <th>TX_DAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2236570.0</td>\n",
       "      <td>37__50</td>\n",
       "      <td>0.004588</td>\n",
       "      <td>0.008694</td>\n",
       "      <td>8.99</td>\n",
       "      <td>2018-09-12 12:41:25</td>\n",
       "      <td>0</td>\n",
       "      <td>14215285.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1485994.0</td>\n",
       "      <td>4833479__17382</td>\n",
       "      <td>0.003989</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>46.06</td>\n",
       "      <td>2018-09-05 23:34:57</td>\n",
       "      <td>0</td>\n",
       "      <td>13649697.0</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1792717.0</td>\n",
       "      <td>7143782__5</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.008694</td>\n",
       "      <td>1020.00</td>\n",
       "      <td>2018-08-16 19:33:50</td>\n",
       "      <td>0</td>\n",
       "      <td>11907230.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6715254.0</td>\n",
       "      <td>4533745__20247</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.008204</td>\n",
       "      <td>62.99</td>\n",
       "      <td>2018-05-21 20:03:22</td>\n",
       "      <td>0</td>\n",
       "      <td>4392202.0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4154156.0</td>\n",
       "      <td>2252003__5</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>53.00</td>\n",
       "      <td>2018-04-19 14:15:54</td>\n",
       "      <td>0</td>\n",
       "      <td>1606554.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CUSTOMER_ID     TERMINAL_ID  X_TERMINAL  Y_TERMINAL  TX_AMOUNT  \\\n",
       "0    2236570.0          37__50    0.004588    0.008694       8.99   \n",
       "1    1485994.0  4833479__17382    0.003989    0.008204      46.06   \n",
       "2    1792717.0      7143782__5    0.009363    0.008694    1020.00   \n",
       "3    6715254.0  4533745__20247    0.000085    0.008204      62.99   \n",
       "4    4154156.0      2252003__5    0.002528    0.005959      53.00   \n",
       "\n",
       "           TX_DATETIME  TX_FRAUD     TX_TIME  TX_DAY  \n",
       "0  2018-09-12 12:41:25         0  14215285.0     164  \n",
       "1  2018-09-05 23:34:57         0  13649697.0     157  \n",
       "2  2018-08-16 19:33:50         0  11907230.0     137  \n",
       "3  2018-05-21 20:03:22         0   4392202.0      50  \n",
       "4  2018-04-19 14:15:54         0   1606554.0      18  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_STRATEGY = 0.18\n",
    "N_JOBS = 10\n",
    "N_TREES = 20\n",
    "N_USERS = 10000\n",
    "N_TERMINALS = 1000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "RANDOM_GRID_RF = {'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_features': [1, 'sqrt', 'log2'], 'max_depth': [5, 16, 28, 40, None], 'min_samples_split': [10, 25, 50], 'min_samples_leaf': [4, 8, 32], 'bootstrap': [True, False]}\n",
    "RANDOM_GRID_RIDGE = {'alpha': [int(x) for x in np.linspace(start = 0.001, stop = 1, num = 100)], 'fit_intercept': [True, False]}\n",
    "RANDOM_GRID_NN = {'hidden_layer_sizes': [int(x) for x in np.linspace(start = 1, stop = 41, num = 80)], 'alpha': [int(x) for x in np.linspace(start = 0.005, stop = 0.02, num = 100)]}\n",
    "\n",
    "\n",
    "# CANDIDATE_REGRESSORS = [MLPRegressor(max_iter=2000, random_state=RANDOM_STATE), Ridge(random_state=RANDOM_STATE), RandomForestRegressor(random_state=RANDOM_STATE)]\n",
    "CANDIDATE_REGRESSORS = [Ridge(random_state=RANDOM_STATE), RandomForestRegressor(random_state=RANDOM_STATE)]\n",
    "# CANDIDATE_GRIDS = [RANDOM_GRID_NN, RANDOM_GRID_RIDGE, RANDOM_GRID_RF]\n",
    "CANDIDATE_GRIDS = [ RANDOM_GRID_RIDGE, RANDOM_GRID_RF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(X_train,y_train,learner, X_test, predictions_proba, discrete_predictions):\n",
    "    learner.fit(X_train, y_train)\n",
    "    y_hat = learner.predict(X_test)\n",
    "    y_hat_proba = learner.predict_proba(X_test)[:,1]\n",
    "    predictions_proba.append(y_hat_proba)\n",
    "    discrete_predictions.append(y_hat)\n",
    "\n",
    "def run_advo(X_train, y_train, window_counter):\n",
    "    advo = ADVO(n_jobs=N_JOBS,sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE, mimo=False)\n",
    "    advo.set_transactions(X_train, y_train)\n",
    "    advo.create_couples()\n",
    "    regressor_scores = advo.select_best_regressor(candidate_regressors=CANDIDATE_REGRESSORS,parameters_set=CANDIDATE_GRIDS)\n",
    "    advo.tune_best_regressors()\n",
    "    advo.fit_regressors()\n",
    "    advo.transactions_df = advo.insert_synthetic_frauds(advo.transactions_df)\n",
    "    regressor_scores.to_csv('results_real/regressor_scores_'+str(window_counter)+'.csv', index=False)\n",
    "    return advo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def make_classification(train_size_days=10, test_size_days=10):\n",
    "\n",
    "    #transactions_df = Generator().generate(filename='dataset_six_months.csv',nb_days_to_generate=180)\n",
    "    #transactions_df = pd.read_csv('utils/dataset_six_months.csv', parse_dates=['TX_DATETIME'])\n",
    "\n",
    "    start_date, end_date = transactions_df['TX_DATETIME'].min(), transactions_df['TX_DATETIME'].max()\n",
    "    start_date_strp = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date_strp = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    window_start_strp, window_end_strp, window_counter  = start_date_strp, start_date_strp + timedelta(days=train_size_days), 0\n",
    "    window_start = str(window_start_strp)\n",
    "    window_end = str(window_end_strp)\n",
    "    while window_end <= end_date:\n",
    "        print('Window: ', window_counter, ' - ', window_start, ' - ', window_end)\n",
    "\n",
    "        # Split data into train and test according to the window\n",
    "        train_mask, test_mask = (transactions_df['TX_DATETIME'] >= window_start) & (transactions_df['TX_DATETIME'] < window_end), (transactions_df['TX_DATETIME'] >= window_end) & (transactions_df['TX_DATETIME'] < str(window_end_strp + timedelta(days=test_size_days)))\n",
    "        \n",
    "        # Drop from test_mask the transactions that have the same cardholder as the frauds in train_mask\n",
    "        training_data, test_data = transactions_df[train_mask], transactions_df[test_mask]\n",
    "        training_frauds = training_data[training_data['TX_FRAUD']==1]\n",
    "        test_data = test_data[~test_data['CUSTOMER_ID'].isin(training_frauds['CUSTOMER_ID'].unique())]\n",
    "\n",
    "        #Create final training and test sets\n",
    "        X_train, y_train, X_test, y_test = training_data.drop(columns=['TX_FRAUD']), training_data['TX_FRAUD'], test_data.drop(columns=['TX_FRAUD']), test_data['TX_FRAUD']\n",
    "        training_variables, predictions_proba, discrete_predictions = ['X_TERMINAL', 'Y_TERMINAL', 'TX_AMOUNT'], [], []\n",
    "\n",
    "        # Oversample data using ADVO, SMOTE, RandomOverSampler and KMeansSMOTE\n",
    "        advo = run_advo(X_train, y_train, window_counter)\n",
    "        kmeans_smote = KMeansSMOTE(n_jobs=N_JOBS, kmeans_estimator=MiniBatchKMeans(n_init=3),sampling_strategy=SAMPLE_STRATEGY, cluster_balance_threshold=0.0025, random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        smote = SMOTE(k_neighbors=NearestNeighbors(n_jobs=N_JOBS),sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        random = RandomOverSampler(sampling_strategy=SAMPLE_STRATEGY, random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        timegan = TimeGANOverSampler(sampling_strategy=SAMPLE_STRATEGY, epochs=100, seq_len=4, n_seq=3, hidden_dim=24, gamma=1, noise_dim = 32, dim = 128, batch_size = 32, log_step = 100, learning_rate = 5e-4,random_state=RANDOM_STATE).fit_resample(X_train[training_variables+['CUSTOMER_ID']], y_train)\n",
    "        ctgan = CTGANOverSampler(sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "    \n",
    "        names = ['Baseline','Baseline_balanced', 'SMOTE','Random', 'KMeansSMOTE', 'CTGAN','TIMEGAN', 'ADVO']\n",
    "        Xy = [(X_train[training_variables], y_train), kmeans_smote, smote, random, ctgan, timegan, (advo.transactions_df[advo.useful_features], advo.transactions_df['TX_FRAUD'])]\n",
    "\n",
    "        fit_predict(X_train[training_variables],y_train, RandomForestClassifier(n_estimators=N_TREES ,n_jobs=N_JOBS, random_state=RANDOM_STATE) , X_test[training_variables], predictions_proba, discrete_predictions)\n",
    "        for X, y in Xy:\n",
    "            fit_predict(X,y, BalancedRandomForestClassifier(n_estimators=N_TREES ,n_jobs=N_JOBS, random_state=RANDOM_STATE) , X_test[training_variables], predictions_proba, discrete_predictions)\n",
    "\n",
    "        # Compute metrics\n",
    "        _, all_metrics = evaluate_models(predictions_proba, discrete_predictions, X_test['CUSTOMER_ID'], names, y_test, K_needed = [50, 100, 200, 500, 1000, 2000])\n",
    "        all_metrics.to_csv('results_real/all_metrics_'+str(window_counter)+'.csv', index=False)\n",
    "        trapzs = compute_kde_difference_auc(Xy, training_variables, names)\n",
    "        trapzs.to_csv('results_real/trapz_'+str(window_counter)+'.csv', index=False)\n",
    "        \n",
    "\n",
    "        window_start_strp, window_end_strp, window_counter  = window_end_strp, window_end_strp + timedelta(days=train_size_days), window_counter + 1\n",
    "        window_end = str(window_end_strp)\n",
    "        window_start = str(window_start_strp)\n",
    "        print('Window ', window_counter, ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window:  0  -  2018-04-01 01:00:02  -  2018-04-08 01:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-16 10:04:00.061597: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-16 10:04:00.061731: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-16 10:04:00.061783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "2023-04-16 10:04:00.089935: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Emddeding network training: 100%|█████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.85it/s]\n",
      "Supervised network training: 100%|████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.52it/s]\n",
      "Joint networks training: 100%|████████████████████████████████████████████████████████| 100/100 [02:08<00:00,  1.29s/it]\n",
      "Synthetic data generation: 100%|████████████████████████████████████████████████████| 3886/3886 [11:35<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window  1  done\n",
      "Window:  1  -  2018-04-08 01:00:02  -  2018-04-15 01:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|█████████████████████████████████████████████████████| 100/100 [00:14<00:00,  7.05it/s]\n",
      "Supervised network training: 100%|████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.67it/s]\n",
      "Joint networks training: 100%|████████████████████████████████████████████████████████| 100/100 [02:10<00:00,  1.31s/it]\n",
      "Synthetic data generation:  97%|██████████████████████████████████████████████████▌ | 3669/3774 [10:50<00:18,  5.76it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Synthetic data generation: 100%|████████████████████████████████████████████████████| 3963/3963 [11:52<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window  3  done\n",
      "Window:  3  -  2018-04-22 01:00:02  -  2018-04-29 01:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|█████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.60it/s]\n",
      "Supervised network training: 100%|████████████████████████████████████████████████████| 100/100 [00:10<00:00,  9.68it/s]\n",
      "Joint networks training: 100%|████████████████████████████████████████████████████████| 100/100 [02:11<00:00,  1.32s/it]\n",
      "Synthetic data generation:  54%|████████████████████████████                        | 2096/3882 [06:13<05:18,  5.61it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Synthetic data generation: 100%|████████████████████████████████████████████████████| 4147/4147 [12:32<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window  5  done\n",
      "Window:  5  -  2018-05-06 01:00:02  -  2018-05-13 01:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|█████████████████████████████████████████████████████| 100/100 [00:18<00:00,  5.31it/s]\n",
      "Supervised network training: 100%|████████████████████████████████████████████████████| 100/100 [00:11<00:00,  8.54it/s]\n",
      "Joint networks training: 100%|████████████████████████████████████████████████████████| 100/100 [02:11<00:00,  1.32s/it]\n",
      "Synthetic data generation: 100%|████████████████████████████████████████████████████| 3768/3768 [11:14<00:00,  5.58it/s]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "make_classification(train_size_days=7, test_size_days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_days=10\n",
    "timedelta(days=train_size_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_transactions_df = pd.read_csv('utils/dataset_six_months.csv', parse_dates=['TX_DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_transactions_df.columns, transactions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column for column in synthetic_transactions_df.columns if column not in transactions_df.columns], [column for column in transactions_df.columns if column not in synthetic_transactions_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tr=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraufs_df = tr[tr['TX_FRAUD'] == 1]\n",
    "fraufs_df[fraufs_df['TERM_COUNTRY']=='SLV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraufs_df = tr[tr['TX_FRAUD'] == 1]['TERM_COUNTRY'].value_counts()['SLV']\n",
    "fraufs_df#[fraufs_df['TERM_MCC']=='9999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7929  VS 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
