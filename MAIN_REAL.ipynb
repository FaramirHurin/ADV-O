{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_seq_items = 100\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler, KMeansSMOTE\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "from ADVO.generator import Generator\n",
    "from ADVO.oversampler import ADVO, TimeGANOverSampler, CTGANOverSampler\n",
    "from ADVO.utils import evaluate_models, compute_kde_difference_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 10 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports for this notebook\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# For Pandas parallelisation\n",
    "from pandarallel import pandarallel\n",
    "# pandarallel.initialize(nb_workers=20)\n",
    "pandarallel.initialize( use_memory_fs=False, nb_workers=10) # \n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/shared_functions_basic.ipynb\n",
    "%run ../worldline_home/shared_functions_basic.ipynb\n",
    "#%run ../../../worldline_home/worldline_home/shared_functions_basic.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['TERM_COUNTRY', 'TERM_MCC']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a set of pickle files, put them together in a single dataframe, and order them by time\n",
    "# It takes as input the folder DIR_INPUT where the files are stored, and the BEGIN_DATE and END_DATE\n",
    "def read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    \n",
    "    files = [join(DIR_INPUT, f) for f in listdir(DIR_INPUT) if f>=BEGIN_DATE+'.pkl' and f<=END_DATE+'.pkl']\n",
    "\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        df = pd.read_pickle(f)\n",
    "        frames.append(df)\n",
    "        del df\n",
    "    df_final = pd.concat(frames)\n",
    "    \n",
    "    df_final=df_final.sort_values('TX_ID')\n",
    "    df_final.reset_index(drop=True,inplace=True)\n",
    "    #  Note: -1 are missing values for real world data \n",
    "    #df_final=df_final.replace([-1],0)\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def clean_categorical(transactions_df):\n",
    "    \n",
    "    all_features = transactions_df.columns\n",
    "\n",
    "    prel_df = transactions_df[transactions_df['TX_FRAUD']==1]\n",
    "    #print(prel_df.loc[prel_df['TERM_COUNTRY']=='SLV'])\n",
    "    prel_df_gen = transactions_df[transactions_df['TX_FRAUD']==0] \n",
    "    categorical = ['TERM_COUNTRY', 'TERM_MCC']\n",
    "    \n",
    "    #print('Expected number is ' + str(prel_df['TERM_COUNTRY'].value_counts()['SLV']))\n",
    "    \n",
    "    features_counts = {}\n",
    "    features_counts_gen = {}\n",
    "    \n",
    "    for column in categorical:\n",
    "        print(column)\n",
    "        features_counts[column] =  prel_df[column].value_counts()\n",
    "        features_counts_gen[column] =  prel_df_gen[column].value_counts()\n",
    "    #print(features_counts['TERM_COUNTRY']['SLV'])\n",
    "    values_to_keep = {}\n",
    "    percentages = {}\n",
    "\n",
    "    counter = 0 \n",
    "    sum_freq = 0\n",
    "\n",
    "    #print('Feature counts terminals columns are ' + str(features_counts['TERM_COUNTRY'].keys()))\n",
    "    #print('Single features are:')\n",
    "    for feature in features_counts.keys():\n",
    "        values_to_keep[feature] = []\n",
    "        percentages[feature] = {}\n",
    "        frequencies = 0\n",
    "        #print(features_counts[feature].keys())\n",
    "        for subkey in features_counts[feature].keys():\n",
    "            # print('Subkey is '+str(subkey) + ', values are ' + str(features_counts_gen[feature][subkey]) )\n",
    "            try:\n",
    "                frequency = features_counts[feature][subkey]/prel_df_gen.shape[0]\n",
    "            except:\n",
    "                print('We are looking into ' + str(features_counts[feature]) + ', for subkey ' + str(subkey) )\n",
    "                \n",
    "                frequency = features_counts[feature][subkey]/prel_df_gen.shape[0]\n",
    "            \n",
    "            values_to_keep[feature].append(subkey)\n",
    "            if subkey in   features_counts_gen[feature].keys():\n",
    "                percentages[feature][subkey] =    features_counts[feature][subkey] / (features_counts[feature][subkey] + features_counts_gen[feature][subkey])\n",
    "            else: \n",
    "                percentages[feature][subkey] =    1\n",
    "            #else:\n",
    "            #    sum_freq +=  (features_counts[feature][subkey] /  features_counts_gen[feature][subkey]) * frequency\n",
    "            #    frequencies += frequency\n",
    "            #    counter = counter + 1\n",
    "        # if counter >0:\n",
    "        for subkey in features_counts_gen[feature].keys():\n",
    "            if subkey not in percentages[feature].keys():\n",
    "                percentages[feature][subkey] = 0\n",
    "    #print(percentages)\n",
    "    #Scale percentages                \n",
    "    for key in percentages.keys():\n",
    "        max_val = max(percentages[key].values())\n",
    "        percentages[key] = {k: v/max_val for k, v in percentages[key].items()}\n",
    "        \n",
    "    for column in categorical:\n",
    "        transactions_df.replace(percentages[column], inplace=True)    \n",
    "    return transactions_df\n",
    "\n",
    "\n",
    "\n",
    "def retrieve_and_clean_realData(DIR_INPUT, BEGIN_DATE, END_DATE):\n",
    "    df = read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
    "    df.rename({'TX_TIME_DAYS': 'TX_DAY', 'TX_TIME_SECONDS': 'TX_TIME'}, inplace=True, axis = 1)\n",
    "    df.drop(['TX_3D_SECURE', 'TX_LOCAL_AMOUNT',\n",
    "        'TX_LOCAL_CURRENCY', 'TX_CARD_ENTRY_MODE', 'CARD_AUTHENTICATION','TX_INTL', 'AGE', 'LANGUAGE', 'GENDER', 'BROKER', 'ZIP',\n",
    "        'INS_CODE', 'CITY', 'COUNTRY', 'PROVINCE_CODE', 'DISTRICT_CODE',\n",
    "        'CARD_BRAND', 'CARD_EXPIRY', 'CARD_TYPE', 'CREDIT_LIMIT', 'TX_ECOM_IND', 'TX_ID'], axis=1, inplace=True)\n",
    "    df = clean_categorical(df)\n",
    "    df.rename({'TERM_MCC': 'X_TERMINAL', 'TERM_COUNTRY': 'Y_TERMINAL'}, inplace=True, axis = 1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  files\n",
      "TERM_COUNTRY\n",
      "TERM_MCC\n",
      "CPU times: user 36min 3s, sys: 1min 22s, total: 37min 25s\n",
      "Wall time: 37min 22s\n"
     ]
    }
   ],
   "source": [
    "DIR_INPUT ='../worldline_home/2018/baseline/data_clean/'\n",
    "#DIR_INPUT='../worldline_home/media/hdd3/worldline_home/2018/baseline/data_clean/' \n",
    "#DIR_INPUT = '2018/baseline/data_clean/'\n",
    "\n",
    "BEGIN_DATE = \"2018-04-01\"\n",
    "END_DATE = \"2018-04-20\"\n",
    "# To load everything:\n",
    "# END_DATE = \"2018-09-30\"\n",
    "\n",
    "#BEGIN_DATE = \"2018-07-25\"\n",
    "#END_DATE = \"2018-08-31\"\n",
    "\n",
    "print(\"Load  files\")\n",
    "%time transactions_df=retrieve_and_clean_realData(DIR_INPUT, BEGIN_DATE, END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_STRATEGY = 0.18\n",
    "N_JOBS = 10\n",
    "N_TREES = 20\n",
    "N_USERS = 10000\n",
    "N_TERMINALS = 1000\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "RANDOM_GRID_RF = {'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], 'max_features': [1, 'sqrt', 'log2'], 'max_depth': [5, 16, 28, 40, None], 'min_samples_split': [10, 25, 50], 'min_samples_leaf': [4, 8, 32], 'bootstrap': [True, False]}\n",
    "RANDOM_GRID_RIDGE = {'alpha': [int(x) for x in np.linspace(start = 0.001, stop = 1, num = 100)], 'fit_intercept': [True, False]}\n",
    "RANDOM_GRID_NN = {'hidden_layer_sizes': [int(x) for x in np.linspace(start = 1, stop = 41, num = 80)], 'alpha': [int(x) for x in np.linspace(start = 0.005, stop = 0.02, num = 100)]}\n",
    "\n",
    "\n",
    "CANDIDATE_REGRESSORS = [MLPRegressor(max_iter=2000, random_state=RANDOM_STATE), Ridge(random_state=RANDOM_STATE), RandomForestRegressor(random_state=RANDOM_STATE)]\n",
    "CANDIDATE_GRIDS = [RANDOM_GRID_NN, RANDOM_GRID_RIDGE, RANDOM_GRID_RF]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(X_train,y_train,learner, X_test, predictions_proba, discrete_predictions):\n",
    "    learner.fit(X_train, y_train)\n",
    "    y_hat = learner.predict(X_test)\n",
    "    y_hat_proba = learner.predict_proba(X_test)[:,1]\n",
    "    predictions_proba.append(y_hat_proba)\n",
    "    discrete_predictions.append(y_hat)\n",
    "\n",
    "def run_advo(X_train, y_train, window_counter):\n",
    "    advo = ADVO(n_jobs=N_JOBS,sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE, mimo=False)\n",
    "    advo.set_transactions(X_train, y_train)\n",
    "    advo.create_couples()\n",
    "    regressor_scores = advo.select_best_regressor(candidate_regressors=CANDIDATE_REGRESSORS,parameters_set=CANDIDATE_GRIDS)\n",
    "    advo.tune_best_regressors()\n",
    "    advo.fit_regressors()\n",
    "    advo.transactions_df = advo.insert_synthetic_frauds(advo.transactions_df)\n",
    "    regressor_scores.to_csv('realresults/regressor_scores_'+str(window_counter)+'.csv', index=False)\n",
    "    return advo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classification(train_size_days=5, test_size_days=2):\n",
    "\n",
    "    #transactions_df = Generator().generate(filename='dataset_six_months.csv',nb_days_to_generate=180)\n",
    "    #transactions_df = pd.read_csv('utils/dataset_six_months.csv', parse_dates=['TX_DATETIME'])\n",
    "\n",
    "    start_date, end_date = transactions_df['TX_DATETIME'].min(), transactions_df['TX_DATETIME'].max()\n",
    "    \n",
    "    window_start, window_end, window_counter  = start_date, start_date + timedelta(days=train_size_days), 0\n",
    "    while window_end <= end_date:\n",
    "        print('Window: ', window_counter, ' - ', window_start, ' - ', window_end)\n",
    "\n",
    "        # Split data into train and test according to the window\n",
    "        train_mask, test_mask = (transactions_df['TX_DATETIME'] >= window_start) & (transactions_df['TX_DATETIME'] < window_end), (transactions_df['TX_DATETIME'] >= window_end) & (transactions_df['TX_DATETIME'] < window_end + timedelta(days=test_size_days))\n",
    "        X_train, y_train, X_test, y_test = transactions_df[train_mask].drop(columns=['TX_FRAUD']), transactions_df[train_mask]['TX_FRAUD'], transactions_df[test_mask].drop(columns=['TX_FRAUD']), transactions_df[test_mask]['TX_FRAUD']\n",
    "        training_variables, predictions_proba, discrete_predictions = ['X_TERMINAL', 'Y_TERMINAL', 'TX_AMOUNT'], [], []\n",
    "\n",
    "        # Oversample data using ADVO, SMOTE, RandomOverSampler and KMeansSMOTE\n",
    "        advo = run_advo(X_train, y_train, window_counter)\n",
    "        kmeans_smote = KMeansSMOTE(n_jobs=N_JOBS, kmeans_estimator=MiniBatchKMeans(n_init=3),sampling_strategy=SAMPLE_STRATEGY, cluster_balance_threshold=0.005, random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        smote = SMOTE(k_neighbors=NearestNeighbors(n_jobs=N_JOBS),sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        random = RandomOverSampler(sampling_strategy=SAMPLE_STRATEGY, random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "        timegan = TimeGANOverSampler(sampling_strategy=SAMPLE_STRATEGY, epochs=100, seq_len=4, n_seq=3, hidden_dim=24, gamma=1, noise_dim = 32, dim = 128, batch_size = 32, log_step = 100, learning_rate = 5e-4,random_state=RANDOM_STATE).fit_resample(X_train[training_variables+['CUSTOMER_ID']], y_train)\n",
    "        ctgan = CTGANOverSampler(sampling_strategy=SAMPLE_STRATEGY,random_state=RANDOM_STATE).fit_resample(X_train[training_variables], y_train)\n",
    "    \n",
    "        names = ['Baseline','Baseline_balanced', 'SMOTE','Random', 'KMeansSMOTE', 'CTGAN','TIMEGAN', 'ADVO']\n",
    "        Xy = [(X_train[training_variables], y_train), kmeans_smote, smote, random, ctgan, timegan, (advo.transactions_df[advo.useful_features], advo.transactions_df['TX_FRAUD'])]\n",
    "\n",
    "        fit_predict(X_train[training_variables],y_train, RandomForestClassifier(n_estimators=N_TREES ,n_jobs=N_JOBS, random_state=RANDOM_STATE) , X_test[training_variables], predictions_proba, discrete_predictions)\n",
    "        for X, y in Xy:\n",
    "            fit_predict(X,y, BalancedRandomForestClassifier(n_estimators=N_TREES ,n_jobs=N_JOBS, random_state=RANDOM_STATE) , X_test[training_variables], predictions_proba, discrete_predictions)\n",
    "\n",
    "        # Compute metrics\n",
    "        _, all_metrics = evaluate_models(predictions_proba, discrete_predictions, X_test['CUSTOMER_ID'], names, y_test, K_needed = [50, 100, 200, 500, 1000, 2000])\n",
    "        all_metrics.to_csv('realresults/all_metrics_'+str(window_counter)+'.csv', index=False)\n",
    "        trapzs = compute_kde_difference_auc(Xy, training_variables, names)\n",
    "        trapzs.to_csv('realresults/trapz_'+str(window_counter)+'.csv', index=False)\n",
    "        \n",
    "\n",
    "        window_start, window_end, window_counter  = window_end, window_end + timedelta(days=train_size_days), window_counter + 1\n",
    "        print('Window ', window_counter, ' done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window:  0  -  2018-04-01 01:00:02  -  2018-04-06 01:00:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Emddeding network training: 100%|████████████████████| 100/100 [00:18<00:00,  5.43it/s]\n",
      "Supervised network training: 100%|███████████████████| 100/100 [00:13<00:00,  7.68it/s]\n",
      "Joint networks training: 100%|███████████████████████| 100/100 [03:13<00:00,  1.93s/it]\n",
      "Synthetic data generation: 100%|███████████████████| 2902/2902 [13:30<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window  1  done\n",
      "Window:  1  -  2018-04-06 01:00:02  -  2018-04-11 01:00:02\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No clusters found with sufficient samples of class 1. Try lowering the cluster_balance_threshold or increasing the number of clusters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(RANDOM_STATE)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmake_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_size_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 19\u001b[0m, in \u001b[0;36mmake_classification\u001b[0;34m(train_size_days, test_size_days)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Oversample data using ADVO, SMOTE, RandomOverSampler and KMeansSMOTE\u001b[39;00m\n\u001b[1;32m     18\u001b[0m advo \u001b[38;5;241m=\u001b[39m run_advo(X_train, y_train, window_counter)\n\u001b[0;32m---> 19\u001b[0m kmeans_smote \u001b[38;5;241m=\u001b[39m \u001b[43mKMeansSMOTE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_JOBS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMiniBatchKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSAMPLE_STRATEGY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_balance_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRANDOM_STATE\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtraining_variables\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(k_neighbors\u001b[38;5;241m=\u001b[39mNearestNeighbors(n_jobs\u001b[38;5;241m=\u001b[39mN_JOBS),sampling_strategy\u001b[38;5;241m=\u001b[39mSAMPLE_STRATEGY,random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE)\u001b[38;5;241m.\u001b[39mfit_resample(X_train[training_variables], y_train)\n\u001b[1;32m     21\u001b[0m random \u001b[38;5;241m=\u001b[39m RandomOverSampler(sampling_strategy\u001b[38;5;241m=\u001b[39mSAMPLE_STRATEGY, random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE)\u001b[38;5;241m.\u001b[39mfit_resample(X_train[training_variables], y_train)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/imblearn/base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/imblearn/base.py:88\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m     86\u001b[0m )\n\u001b[0;32m---> 88\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     94\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/imblearn/over_sampling/_smote/cluster.py:265\u001b[0m, in \u001b[0;36mKMeansSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    262\u001b[0m cluster_weights \u001b[38;5;241m=\u001b[39m cluster_sparsities \u001b[38;5;241m/\u001b[39m cluster_sparsities\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid_clusters:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo clusters found with sufficient samples of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_sample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Try lowering the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster_balance_threshold or increasing the number of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclusters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     )\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m valid_cluster_idx, valid_cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(valid_clusters):\n\u001b[1;32m    273\u001b[0m     X_cluster \u001b[38;5;241m=\u001b[39m _safe_indexing(X, valid_cluster)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No clusters found with sufficient samples of class 1. Try lowering the cluster_balance_threshold or increasing the number of clusters."
     ]
    }
   ],
   "source": [
    "np.random.seed(RANDOM_STATE)\n",
    "    \n",
    "make_classification(train_size_days=5, test_size_days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_transactions_df = pd.read_csv('utils/dataset_six_months.csv', parse_dates=['TX_DATETIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_transactions_df.columns, transactions_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[column for column in synthetic_transactions_df.columns if column not in transactions_df.columns], [column for column in transactions_df.columns if column not in synthetic_transactions_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tr=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraufs_df = tr[tr['TX_FRAUD'] == 1]\n",
    "fraufs_df[fraufs_df['TERM_COUNTRY']=='SLV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraufs_df = tr[tr['TX_FRAUD'] == 1]['TERM_COUNTRY'].value_counts()['SLV']\n",
    "fraufs_df#[fraufs_df['TERM_MCC']=='9999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7929  VS 9999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
