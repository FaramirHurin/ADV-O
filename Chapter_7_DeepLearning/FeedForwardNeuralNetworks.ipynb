{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lVqmFNktUM0"
      },
      "source": [
        "(FeedForwardNeuralNetworks)=\n",
        "# Feed-forward neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLuYuOyntUM2"
      },
      "source": [
        "As neural networks are a pillar in both the early and the recent advances of artificial intelligence, their use for credit card fraud detection is not surprising. The first examples of simple feed-forward neural networks applied to fraud detection can bring us back to the early 90s {cite}`ghosh1994credit,aleskerov1997cardwatch`. Naturally, in recent FDS studies, neural networks are often found in experimental benchmarks, along with random forests, XGBoost, or logistic regression.\n",
        "\n",
        "At the core of a feed-forward neural network is the artificial neuron, a simple machine learning model that consists of a linear combination of input variables followed by the application of an activation function $\\sigma$ (sigmoid, ReLU, tanh, ...). More precisely, given a list of $n$ input variables $x_i$, the output $h$ of the artificial neuron is computed as follows:\n",
        "\n",
        "$h = \\sigma(\\sum_{i=1}^n w_i*x_i)$\n",
        "\n",
        "where $w_i$ are the weights of the model.\n",
        "\n",
        "A whole network is composed of a succession of layers containing neurons that take, as inputs, the output values of the previous layer.\n",
        "\n",
        "![A feed forward neural network architecture](https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook/blob/main/Chapter_7_DeepLearning/images/neuralnetwork.png?raw=1)\n",
        "\n",
        "When applied to the fraud detection problem, the architecture is designed as follows:\n",
        "\n",
        "* At the beginning of the network, the neurons take as input the characteristics of a credit card transaction, i.e. the features that were defined in the previous chapters.\n",
        "* At the end, the network outputs a single neuron that aims at representing the probability for the input transaction to be a fraud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRALxYNPtUM3"
      },
      "source": [
        "The rest of the architecture (other layers), the neurons specificity (activation functions), and other hyperparameters (optimization, data processing, ...) are left to the practitioner's choice.\n",
        "\n",
        "The most popular training algorithm for feedforward architectures is backpropagation {cite}`hecht1992theory`. The idea is to iterate over all samples of the dataset and perform two key operations:\n",
        "* the forward pass: setting the sample's features values in the input neurons and computing all the layers to finally obtain a predicted output.\n",
        "* the backward pass: computing a cost function, i.e. a discrepancy between the prediction and the expected ground truth output, and trying to minimize it with an optimizer (e.g. gradient descent) by updating weights layer after layer, from output to input.\n",
        "\n",
        "This section covers the design of a feed-foward neural network for fraud detection. It describes how to:\n",
        "* Implement a first simple neural network and study the impact of several architectures and design choices.\n",
        "* Wrap it to make it compatible with the model selection methodology from Chapter 5 and run a grid-search to select its optimal parameters.\n",
        "* Store the important functions for a final comparison between deep learning techniques and other baselines at the end of the chapter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtNyI2XXtUM3"
      },
      "source": [
        "Let us first start by importing all the necessary libraries and functions and retrieving the simulated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "scrolled": true,
        "tags": [
          "hide-cell"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "VbSEOaSKtUM3",
        "outputId": "83f2dec1-f724-4590-b485-feece1e88e6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 63257  100 63257    0     0   128k      0 --:--:-- --:--:-- --:--:--  128k\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialization: Load shared functions and simulated data\n",
        "\n",
        "# Load shared functions\n",
        "!curl -O https://raw.githubusercontent.com/Fraud-Detection-Handbook/fraud-detection-handbook/main/Chapter_References/shared_functions.py\n",
        "%run shared_functions.py\n",
        "\n",
        "# Get simulated data from Github repository\n",
        "if not os.path.exists(\"simulated-data-transformed\"):\n",
        "    !git clone https://github.com/Fraud-Detection-Handbook/simulated-data-transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaUdE4ivtUM4"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ags-iQBXtUM5"
      },
      "source": [
        "The experimental setup is the same as in Chapter 5. More precisely, at the end of the chapter, model selection will be based on a grid search with multiple validations. Each time, one week of data will be used for training a neural network and one week of data for testing the predictions.\n",
        "\n",
        "To implement the first base network and explore several architecture choices, let us start by selecting a training and validation period arbitrarily. The experiments will be based on the transformed simulated data (`simulated-data-transformed/data/`) and the same feature set as other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSWGjEUetUM5",
        "outputId": "fd771e96-d419-4398-ecca-fd9f6d7d7de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load  files\n",
            "CPU times: user 430 ms, sys: 764 ms, total: 1.19 s\n",
            "Wall time: 1.19 s\n",
            "919767 transactions loaded, containing 8195 fraudulent transactions\n"
          ]
        }
      ],
      "source": [
        "DIR_INPUT='simulated-data-transformed/data/'\n",
        "\n",
        "BEGIN_DATE = \"2018-06-11\"\n",
        "END_DATE = \"2018-09-14\"\n",
        "\n",
        "print(\"Load  files\")\n",
        "%time transactions_df=read_from_files(DIR_INPUT, BEGIN_DATE, END_DATE)\n",
        "print(\"{0} transactions loaded, containing {1} fraudulent transactions\".format(len(transactions_df),transactions_df.TX_FRAUD.sum()))\n",
        "\n",
        "output_feature=\"TX_FRAUD\"\n",
        "\n",
        "input_features=['TX_AMOUNT','TX_DURING_WEEKEND', 'TX_DURING_NIGHT', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW',\n",
        "       'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW', 'CUSTOMER_ID_NB_TX_7DAY_WINDOW',\n",
        "       'CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW', 'CUSTOMER_ID_NB_TX_30DAY_WINDOW',\n",
        "       'CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW', 'TERMINAL_ID_NB_TX_1DAY_WINDOW',\n",
        "       'TERMINAL_ID_RISK_1DAY_WINDOW', 'TERMINAL_ID_NB_TX_7DAY_WINDOW',\n",
        "       'TERMINAL_ID_RISK_7DAY_WINDOW', 'TERMINAL_ID_NB_TX_30DAY_WINDOW',\n",
        "       'TERMINAL_ID_RISK_30DAY_WINDOW']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "TIwPVKIttUM5"
      },
      "outputs": [],
      "source": [
        "# Setting the starting day for the training period, and the deltas\n",
        "start_date_training = datetime.datetime.strptime(\"2018-07-25\", \"%Y-%m-%d\")\n",
        "delta_train=7\n",
        "delta_delay=7\n",
        "delta_test=7\n",
        "(train_df, test_df)=get_train_test_set(transactions_df,start_date_training,\n",
        "                                       delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
        "# By default, scaling the input data\n",
        "(train_df, test_df)=scaleData(train_df,test_df,input_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "z5PAqWqCCw5U",
        "outputId": "0ce5f5bb-b308-45d7-cea0-e62dfc84cf21"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        TRANSACTION_ID         TX_DATETIME  CUSTOMER_ID  TERMINAL_ID  \\\n",
              "421597         1102483 2018-07-25 00:00:29         1111         2328   \n",
              "421598         1102484 2018-07-25 00:01:08          676         6846   \n",
              "421599         1102485 2018-07-25 00:01:35          402         4771   \n",
              "421600         1102486 2018-07-25 00:01:43         4218          863   \n",
              "421601         1102487 2018-07-25 00:02:26         3711         3599   \n",
              "\n",
              "        TX_AMOUNT  TX_TIME_SECONDS  TX_TIME_DAYS  TX_FRAUD  TX_FRAUD_SCENARIO  \\\n",
              "421597  -0.303663          9936029           115         0                  0   \n",
              "421598  -1.042495          9936068           115         0                  0   \n",
              "421599   0.663578          9936095           115         0                  0   \n",
              "421600  -0.722769          9936103           115         0                  0   \n",
              "421601   0.134655          9936146           115         0                  0   \n",
              "\n",
              "        TX_DURING_WEEKEND  ...  CUSTOMER_ID_NB_TX_7DAY_WINDOW  \\\n",
              "421597           -0.62823  ...                       0.655714   \n",
              "421598           -0.62823  ...                      -0.510235   \n",
              "421599           -0.62823  ...                      -1.028434   \n",
              "421600           -0.62823  ...                       0.655714   \n",
              "421601           -0.62823  ...                       1.044364   \n",
              "\n",
              "        CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW  CUSTOMER_ID_NB_TX_30DAY_WINDOW  \\\n",
              "421597                           -0.800291                        0.934422   \n",
              "421598                           -1.129245                       -0.585080   \n",
              "421599                            4.267868                       -0.964955   \n",
              "421600                           -1.062067                        0.520012   \n",
              "421601                            0.878439                        1.107093   \n",
              "\n",
              "        CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW  TERMINAL_ID_NB_TX_1DAY_WINDOW  \\\n",
              "421597                            -0.801283                       0.000248   \n",
              "421598                            -1.206503                       0.980006   \n",
              "421599                             1.988988                      -0.979511   \n",
              "421600                            -1.147390                       0.980006   \n",
              "421601                             0.699063                       0.000248   \n",
              "\n",
              "        TERMINAL_ID_RISK_1DAY_WINDOW  TERMINAL_ID_NB_TX_7DAY_WINDOW  \\\n",
              "421597                     -0.075836                       1.005829   \n",
              "421598                     -0.075836                      -0.664368   \n",
              "421599                     -0.075836                       1.673908   \n",
              "421600                     -0.075836                      -0.330328   \n",
              "421601                     -0.075836                       1.005829   \n",
              "\n",
              "        TERMINAL_ID_RISK_7DAY_WINDOW  TERMINAL_ID_NB_TX_30DAY_WINDOW  \\\n",
              "421597                      -0.11808                        0.241363   \n",
              "421598                      -0.11808                       -0.363745   \n",
              "421599                      -0.11808                        0.846470   \n",
              "421600                      -0.11808                       -0.968852   \n",
              "421601                      -0.11808                        0.967491   \n",
              "\n",
              "        TERMINAL_ID_RISK_30DAY_WINDOW  \n",
              "421597                      -0.147825  \n",
              "421598                      -0.147825  \n",
              "421599                      -0.147825  \n",
              "421600                      -0.147825  \n",
              "421601                      -0.147825  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4058e82e-0cc5-4e51-9604-8a5a57fd2432\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TRANSACTION_ID</th>\n",
              "      <th>TX_DATETIME</th>\n",
              "      <th>CUSTOMER_ID</th>\n",
              "      <th>TERMINAL_ID</th>\n",
              "      <th>TX_AMOUNT</th>\n",
              "      <th>TX_TIME_SECONDS</th>\n",
              "      <th>TX_TIME_DAYS</th>\n",
              "      <th>TX_FRAUD</th>\n",
              "      <th>TX_FRAUD_SCENARIO</th>\n",
              "      <th>TX_DURING_WEEKEND</th>\n",
              "      <th>...</th>\n",
              "      <th>CUSTOMER_ID_NB_TX_7DAY_WINDOW</th>\n",
              "      <th>CUSTOMER_ID_AVG_AMOUNT_7DAY_WINDOW</th>\n",
              "      <th>CUSTOMER_ID_NB_TX_30DAY_WINDOW</th>\n",
              "      <th>CUSTOMER_ID_AVG_AMOUNT_30DAY_WINDOW</th>\n",
              "      <th>TERMINAL_ID_NB_TX_1DAY_WINDOW</th>\n",
              "      <th>TERMINAL_ID_RISK_1DAY_WINDOW</th>\n",
              "      <th>TERMINAL_ID_NB_TX_7DAY_WINDOW</th>\n",
              "      <th>TERMINAL_ID_RISK_7DAY_WINDOW</th>\n",
              "      <th>TERMINAL_ID_NB_TX_30DAY_WINDOW</th>\n",
              "      <th>TERMINAL_ID_RISK_30DAY_WINDOW</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>421597</th>\n",
              "      <td>1102483</td>\n",
              "      <td>2018-07-25 00:00:29</td>\n",
              "      <td>1111</td>\n",
              "      <td>2328</td>\n",
              "      <td>-0.303663</td>\n",
              "      <td>9936029</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.62823</td>\n",
              "      <td>...</td>\n",
              "      <td>0.655714</td>\n",
              "      <td>-0.800291</td>\n",
              "      <td>0.934422</td>\n",
              "      <td>-0.801283</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>-0.075836</td>\n",
              "      <td>1.005829</td>\n",
              "      <td>-0.11808</td>\n",
              "      <td>0.241363</td>\n",
              "      <td>-0.147825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421598</th>\n",
              "      <td>1102484</td>\n",
              "      <td>2018-07-25 00:01:08</td>\n",
              "      <td>676</td>\n",
              "      <td>6846</td>\n",
              "      <td>-1.042495</td>\n",
              "      <td>9936068</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.62823</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.510235</td>\n",
              "      <td>-1.129245</td>\n",
              "      <td>-0.585080</td>\n",
              "      <td>-1.206503</td>\n",
              "      <td>0.980006</td>\n",
              "      <td>-0.075836</td>\n",
              "      <td>-0.664368</td>\n",
              "      <td>-0.11808</td>\n",
              "      <td>-0.363745</td>\n",
              "      <td>-0.147825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421599</th>\n",
              "      <td>1102485</td>\n",
              "      <td>2018-07-25 00:01:35</td>\n",
              "      <td>402</td>\n",
              "      <td>4771</td>\n",
              "      <td>0.663578</td>\n",
              "      <td>9936095</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.62823</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.028434</td>\n",
              "      <td>4.267868</td>\n",
              "      <td>-0.964955</td>\n",
              "      <td>1.988988</td>\n",
              "      <td>-0.979511</td>\n",
              "      <td>-0.075836</td>\n",
              "      <td>1.673908</td>\n",
              "      <td>-0.11808</td>\n",
              "      <td>0.846470</td>\n",
              "      <td>-0.147825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421600</th>\n",
              "      <td>1102486</td>\n",
              "      <td>2018-07-25 00:01:43</td>\n",
              "      <td>4218</td>\n",
              "      <td>863</td>\n",
              "      <td>-0.722769</td>\n",
              "      <td>9936103</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.62823</td>\n",
              "      <td>...</td>\n",
              "      <td>0.655714</td>\n",
              "      <td>-1.062067</td>\n",
              "      <td>0.520012</td>\n",
              "      <td>-1.147390</td>\n",
              "      <td>0.980006</td>\n",
              "      <td>-0.075836</td>\n",
              "      <td>-0.330328</td>\n",
              "      <td>-0.11808</td>\n",
              "      <td>-0.968852</td>\n",
              "      <td>-0.147825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421601</th>\n",
              "      <td>1102487</td>\n",
              "      <td>2018-07-25 00:02:26</td>\n",
              "      <td>3711</td>\n",
              "      <td>3599</td>\n",
              "      <td>0.134655</td>\n",
              "      <td>9936146</td>\n",
              "      <td>115</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.62823</td>\n",
              "      <td>...</td>\n",
              "      <td>1.044364</td>\n",
              "      <td>0.878439</td>\n",
              "      <td>1.107093</td>\n",
              "      <td>0.699063</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>-0.075836</td>\n",
              "      <td>1.005829</td>\n",
              "      <td>-0.11808</td>\n",
              "      <td>0.967491</td>\n",
              "      <td>-0.147825</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4058e82e-0cc5-4e51-9604-8a5a57fd2432')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4058e82e-0cc5-4e51-9604-8a5a57fd2432 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4058e82e-0cc5-4e51-9604-8a5a57fd2432');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4db814c6-d9d5-4707-95fb-2b3f786db56f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4db814c6-d9d5-4707-95fb-2b3f786db56f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4db814c6-d9d5-4707-95fb-2b3f786db56f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL6qsXfOtUM6"
      },
      "source": [
        "## Overview of the neural network pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_fz2zTdtUM6"
      },
      "source": [
        "The first step here is to implement a base neural network. There are several Python libraries that we can use (TensorFlow, PyTorch, Keras, MXNet, ...). In this book, the PyTorch library {cite}`paszke2017automatic` is used, but the models and benchmarks that will be developed could also be implemented with other libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "C7Kes7xXtUM6"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0S2lSowtUM6"
      },
      "source": [
        "If torch and cuda libraries are installed properly, the models developed in this chapter can be trained on the GPU. For that, let us create a \"DEVICE\" variable and set it to \"cuda\" if a cuda device is available and \"cpu\" otherwise. In the rest of the chapter, all the models and tensors will be sent to this device for computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1fA1vketUM6",
        "outputId": "2d9bb134-1f30-4139-b586-eb9f9940f170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected device is cpu\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "print(\"Selected device is\",DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU2aV78LtUM7"
      },
      "source": [
        "To ensure reproducibility, a random seed will be fixed like in previous chapters. Additionally to setting the seed for `NumPy` and `random`, it is necessary to set it for `torch`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "VcDA5ngQtUM7"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QqYaTOztUM7"
      },
      "source": [
        "The function `seed_everything` defined above will be run before each model initialization and training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV9LaAn5tUM7"
      },
      "source": [
        "Before diving into the neural network implementation, let us summarize the main elements of a deep learning training/testing pipeline in Torch:\n",
        "* Datasets/Dataloaders: It is recommended to manipulate data with specific PyTorch classes. [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) is the interface to access the data. Given a sample's index, it provides a well-formed input-output for the model. [Dataloader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) takes the Dataset as input and provides an iterator for the training loop. It also allows to create batches, shuffle data, and parallelize data preparation.\n",
        "* Model/Module: Any model in PyTorch is a [torch.module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). It has an init function in which it instantiates all the necessary submodules (layers) and initializes their weights. It also has a forward function that defines all the operations of the forward pass.\n",
        "* The optimizer: The [optimizer](https://pytorch.org/docs/stable/optim.html) is the object that implements the optimization algorithm. It is called after the loss is computed to calculate the necessary model updates. The most basic one is SGD, but there are many others like RMSProp, Adagrad, Adam, ...\n",
        "* Training loop and evaluation: the training loop is the core of a model's training. It consists in performing several iterations (epochs), getting all the training batches from the loader, performing the forward pass, computing the loss, and calling the optimizer. After each epoch, an evaluation can be performed to track the model's evolution and possibly stop the process.\n",
        "\n",
        "The next subsections describe and implement in details each of these elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy_EsNdqtUM7"
      },
      "source": [
        "### Data management: Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D478faOhtUM7"
      },
      "source": [
        "The first step is to convert our data into objects that PyTorch can use, like `FloatTensors`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "la2QCzHYtUM7"
      },
      "outputs": [],
      "source": [
        "x_train = torch.FloatTensor(train_df[input_features].values)\n",
        "x_test = torch.FloatTensor(test_df[input_features].values)\n",
        "y_train = torch.FloatTensor(train_df[output_feature].values)\n",
        "y_test = torch.FloatTensor(test_df[output_feature].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsA9ZWMKtUM7"
      },
      "source": [
        "Next comes the definition of a custom `Dataset`. This dataset is initialized with `x_train`/`x_test` and `y_train`/`y_test` and returns the individual samples in the format required by our model, after sending them to the right device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "VLghLxr1tUM7"
      },
      "outputs": [],
      "source": [
        "class FraudDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "        'Initialization'\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        'Returns the total number of samples'\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample index\n",
        "        if self.y is not None:\n",
        "            return self.x[index].to(DEVICE), self.y[index].to(DEVICE)\n",
        "        else:\n",
        "            return self.x[index].to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qySNFWDtUM8"
      },
      "source": [
        "Note: This first custom Dataset `FraudDataset` seems useless because its role is very limited (simply returning a row from x and y) and because the matrices x and y are already loaded in RAM. This example is provided for educational purposes. But the concept of Dataset has a high interest when sample preparation requires more preprocessing. For instance, it becomes very handy for sequence preparation when using recurrent models (like an LSTM). This will be covered more in-depth later in this chapter but for example, a Dataset for sequential models performs several operations before returning a sample: searching for the history of transactions of the same cardholder and appending it to the current transaction before returning the whole sequence. It avoids preparing all the sequences in advance, which would entail repeating several transactions' features in memory and consuming more RAM than necessary. Datasets objects are also useful when dealing with large image datasets in order to load the images on the fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjJiz5gotUM8"
      },
      "source": [
        "Now that `FraudDataset` is defined, one can choose the training/evaluation parameters and instantiate DataLoaders. For now, let us consider a batch size of 64: this means that at each optimization step, 64 samples will be requested to the Dataset, turned into a batch, and go through the forward pass in parallel. Then the aggregation (sum or average) of the gradient of their losses will be used for backpropagation.\n",
        "\n",
        "For the training DataLoader, the shuffle option will be set to `True` so that the order of the data seen by the model will not be the same from one epoch to another. This is recommended and known to be beneficial in Neural Network training {cite}`ruder2016overview`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "1aTzCzaztUM8"
      },
      "outputs": [],
      "source": [
        "train_loader_params = {'batch_size': 64,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 0}\n",
        "test_loader_params = {'batch_size': 64,\n",
        "          'num_workers': 0}\n",
        "\n",
        "# Generators\n",
        "\n",
        "training_set = FraudDataset(x_train, y_train)\n",
        "\n",
        "testing_set = FraudDataset(x_test, y_test)\n",
        "\n",
        "\n",
        "training_generator = torch.utils.data.DataLoader(training_set, **train_loader_params)\n",
        "testing_generator = torch.utils.data.DataLoader(testing_set, **test_loader_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trKSrf1ftUM8"
      },
      "source": [
        "The `num_workers` parameter allows parallelizing batch preparation. It can be useful when the `Dataset` requires a lot of processing before returning a sample. Here we do not use multiprocessing so we set `num_workers` to 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH9hHqkUtUM8"
      },
      "source": [
        "### The model aka the module\n",
        "\n",
        "After defining the data pipeline, the next step is to design the module. Let us start with a first rather simple feed-forward neural network.\n",
        "\n",
        "As suggested in the introduction, the idea is to define several fully connected layers (`torch.nn.Linear`). A first layer `fc1` which takes as input as many neurons as there are features in the input x. It can be followed by a hidden layer with a chosen number of neurons (`hidden_size`). Finally comes the output layer which has a single output neuron to fit the label (`fraud` or `genuine`, represented by 1 and 0).\n",
        "\n",
        "In the past, the sigmoid activation function used to be the primary choice for all activation functions in all layers of a neural network. Today, the preferred choice is `ReLU` (or variants like `eLU`, `leaky ReLU`), at least for the intermediate neurons. It has empirically proven to be a better choice for optimization and speed {cite}`nair2010rectified`. For output neurons, the choice depends on the range or the expected distribution for the output value to be predicted.\n",
        "\n",
        "Below are plotted the output of several activation functions with respect to their input value to show how they behave and how they are distributed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "k_rdRnn8tUM8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "fig_activation, axs = plt.subplots(3, 2,figsize=(11, 13))\n",
        "\n",
        "input_values = torch.arange(-5, 5, 0.05)\n",
        "\n",
        "#linear activation\n",
        "output_values = input_values\n",
        "axs[0, 0].plot(input_values, output_values)\n",
        "axs[0, 0].set_title('Linear')\n",
        "axs[0, 0].set_ylim([-5.1,5.1])\n",
        "\n",
        "#heavyside activation\n",
        "output_values = input_values>0\n",
        "axs[0, 1].plot(input_values, output_values)\n",
        "axs[0, 1].set_title('Heavyside (perceptron)')\n",
        "axs[0, 1].set_ylim([-0.1,1.1])\n",
        "\n",
        "#sigmoid activation\n",
        "activation = torch.nn.Sigmoid()\n",
        "output_values = activation(input_values)\n",
        "axs[1, 0].plot(input_values, output_values)\n",
        "axs[1, 0].set_title('Sigmoid')\n",
        "axs[1, 0].set_ylim([-1.1,1.1])\n",
        "\n",
        "#tanh activation\n",
        "activation = torch.nn.Tanh()\n",
        "output_values = activation(input_values)\n",
        "axs[1, 1].plot(input_values, output_values)\n",
        "axs[1, 1].set_title('Tanh')\n",
        "axs[1, 1].set_ylim([-1.1,1.1])\n",
        "\n",
        "#relu activation\n",
        "activation = torch.nn.ReLU()\n",
        "output_values = activation(input_values)\n",
        "axs[2, 0].plot(input_values, output_values)\n",
        "axs[2, 0].set_title('ReLU')\n",
        "axs[2, 0].set_ylim([-0.5,5.1])\n",
        "\n",
        "#leaky relu activation\n",
        "activation = torch.nn.LeakyReLU(negative_slope=0.05)\n",
        "output_values = activation(input_values)\n",
        "axs[2, 1].plot(input_values, output_values)\n",
        "axs[2, 1].set_title('Leaky ReLU')\n",
        "axs[2, 1].set_ylim([-0.5,5.1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w3tF7ehCtUM8",
        "outputId": "d0d31f4d-c93c-4a1c-87bc-b56068a00bca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 1100x1300 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "fig_activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgTgphB2tUM8"
      },
      "source": [
        "For our fraud detection neural network, the `ReLU` activation will be used for the hidden layer and a `Sigmoid` activation for the output layer. The first is the primary choice for intermediate layers in deep learning. The latter is the primary choice for the output neurons in binary classification problems because it outputs values between 0 and 1 that can be interpreted as probabilities.\n",
        "\n",
        "To implement this, let us create a new class `SimpleFraudMLP` that will inherit from a torch module. Its layers (`fc1`, `relu`, `fc2`, `sigmoid`) are initialized in the `__init__` function and will be used successively in the forward pass.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "mkoVtl3ktUM9"
      },
      "outputs": [],
      "source": [
        "class SimpleFraudMLP(torch.nn.Module):\n",
        "\n",
        "        def __init__(self, input_size, hidden_size):\n",
        "            super(SimpleFraudMLP, self).__init__()\n",
        "            # parameters\n",
        "            self.input_size = input_size\n",
        "            self.hidden_size  = hidden_size\n",
        "\n",
        "            #input to hidden\n",
        "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "            self.relu = torch.nn.ReLU()\n",
        "            #hidden to output\n",
        "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
        "            self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "        def forward(self, x):\n",
        "\n",
        "            hidden = self.fc1(x)\n",
        "            relu = self.relu(hidden)\n",
        "            output = self.fc2(relu)\n",
        "            output = self.sigmoid(output)\n",
        "\n",
        "            return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqZjn03BtUM9"
      },
      "source": [
        "Once defined, instantiating the model with `1000` neurons in its hidden layer and sending it to the device can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Zt382u0PtUM9"
      },
      "outputs": [],
      "source": [
        "model = SimpleFraudMLP(len(input_features), 1000).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buSdQWj7tUM9"
      },
      "source": [
        "### The optimizer and the training loop\n",
        "\n",
        "Optimization is at the core of neural network training. The above neural network is designed to output a single value between 0 and 1. The goal is that this value gets as close to 1 (resp. 0) as possible for an input describing a fraudulent (resp. genuine) transaction.\n",
        "\n",
        "In practice, this goal is formulated with an optimization problem that aims at minimizing or maximizing some cost/loss function. The role of the loss function is precisely to measure the discrepancy between the predicted value and the expected value (0 or 1), also referred to as the ground truth. There are many loss functions (mean squared error, cross-entropy, KL-divergence, hinge loss, mean absolute error) available in PyTorch, and each serves a specific purpose. Here we only focus on binary cross-entropy because it is the most relevant loss function for binary classification problems like fraud detection. It is defined as follows:\n",
        "\n",
        "$BCE(y,p) = −(y*log(p)+(1−y)*log(1−p))$\n",
        "\n",
        "Where $y$ is the ground truth (in $\\{0,1\\}$) and $p$ the predicted output (in $]0,1[$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "ULbRe3DstUM9"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.BCELoss().to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8EAqBrxtUM9"
      },
      "source": [
        "Note: Pushing the criterion to the device is only required if this one stores/updates internal state variables or has parameters. It is unnecessary but not detrimental in the above case. We do it to show the most general implementation.\n",
        "\n",
        "Before even training the model, one can already measure its initial loss on the testing set. For this, the model has to be put in `eval` mode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v-cI8pQtUM9",
        "outputId": "620a5f1d-7847-46ce-bb43-c0a25118bc47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleFraudMLP(\n",
              "  (fc1): Linear(in_features=15, out_features=1000, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=1000, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj28ZUa0tUM-"
      },
      "source": [
        "Then, the process consists in iterating over the testing generator, making predictions, and evaluating the chosen `criterion` (here `torch.nn.BCELoss`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYPKcGOLtUNF",
        "outputId": "3a8633ea-5213-437e-878d-6d58cd43ac7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6754083204871344"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "def evaluate_model(model,generator,criterion):\n",
        "    model.eval()\n",
        "    batch_losses = []\n",
        "    for x_batch, y_batch in generator:\n",
        "        # Forward pass\n",
        "        y_pred = model(x_batch)\n",
        "        # Compute Loss\n",
        "        loss = criterion(y_pred.squeeze(), y_batch)\n",
        "        batch_losses.append(loss.item())\n",
        "    mean_loss = np.mean(batch_losses)\n",
        "    return mean_loss\n",
        "\n",
        "evaluate_model(model,testing_generator,criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMtIzydxtUNF"
      },
      "source": [
        "Recall that the optimization problem is defined as follows: minimize the total/average binary cross-entropy of the model over all samples from the training dataset. Therefore, training the model consists in applying an optimization algorithm (backpropagation) to numerically solve the optimization problem.\n",
        "\n",
        "The optimization algorithm or `optimizer` can be the standard stochastic gradient descent with a constant learning rate (`torch.optim.SGD`) or with an adaptive learning rate (`torch.optim.Adagrad`, `torch.optim.Adam`, etc...). Several optimization hyperparameters (learning rate, momentum, batch size, ...) can be tuned. Note that choosing the right optimizer and hyperparameters will impact convergence speed and the quality of the reached optimum. Below is an illustration showing how fast different optimizers can reach the optimum (represented with a star) of a two dimensional optimization problem over the training process.\n",
        "\n",
        "![Optimizers convergence](https://ml-cheatsheet.readthedocs.io/en/latest/_images/optimizers.gif)\n",
        "\n",
        "Source: https://cs231n.github.io/neural-networks-3/\n",
        "\n",
        "\n",
        "Here, let us start with the arbitrary choice `SGD`, with a learning rate of `0.07`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "tqxrLI8StUNG"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.07)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l750u1V-tUNG"
      },
      "source": [
        "And let us implement the training loop for our neural network. First, the model has to be set in training mode. Then several iterations can be performed over the training generator (each iteration is called an epoch). During each iteration, a succession of training batches are provided by the generator and a forward pass is performed to get the model's predictions. Then the criterion is computed between predictions and ground truth, and finally, the backward pass is carried out to update the model with the optimizer.\n",
        "Let us start by setting the number of epochs to `150` arbitrarily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "tags": [
          "output_scroll"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wkVDnZXdtUNG",
        "outputId": "f4a7ec15-4de2-4857-c913-cf02450f5171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train loss: 0.03570016137555553\n",
            "test loss: 0.022358749608757663\n",
            "\n",
            "Epoch 1: train loss: 0.026285224329887558\n",
            "test loss: 0.021145284218298584\n",
            "\n",
            "Epoch 2: train loss: 0.024935485474915055\n",
            "test loss: 0.021621913086105667\n",
            "\n",
            "Epoch 3: train loss: 0.024153651457171812\n",
            "test loss: 0.020166814625626132\n",
            "\n",
            "Epoch 4: train loss: 0.023543032519470593\n",
            "test loss: 0.020734394968995408\n",
            "\n",
            "Epoch 5: train loss: 0.02311610374420254\n",
            "test loss: 0.02028386041309386\n",
            "\n",
            "Epoch 6: train loss: 0.022610568783829055\n",
            "test loss: 0.01981479125151065\n",
            "\n",
            "Epoch 7: train loss: 0.022428529804371312\n",
            "test loss: 0.021500718789318276\n",
            "\n",
            "Epoch 8: train loss: 0.021857453960651526\n",
            "test loss: 0.02204068632897104\n",
            "\n",
            "Epoch 9: train loss: 0.021729045080776006\n",
            "test loss: 0.02033687148125112\n",
            "\n",
            "Epoch 10: train loss: 0.021540807152798772\n",
            "test loss: 0.019787190465152295\n",
            "\n",
            "Epoch 11: train loss: 0.021363750270272753\n",
            "test loss: 0.019339693427028626\n",
            "\n",
            "Epoch 12: train loss: 0.0210939898025147\n",
            "test loss: 0.02080311853506088\n",
            "\n",
            "Epoch 13: train loss: 0.02085636487506688\n",
            "test loss: 0.019782864145165807\n",
            "\n",
            "Epoch 14: train loss: 0.020880066195979614\n",
            "test loss: 0.020224935343718047\n",
            "\n",
            "Epoch 15: train loss: 0.020645820632972156\n",
            "test loss: 0.020496649869207014\n",
            "\n",
            "Epoch 16: train loss: 0.02023364249820671\n",
            "test loss: 0.01980759760131804\n",
            "\n",
            "Epoch 17: train loss: 0.02031195261256484\n",
            "test loss: 0.019600149428213278\n",
            "\n",
            "Epoch 18: train loss: 0.020111929943947233\n",
            "test loss: 0.019799932609511586\n",
            "\n",
            "Epoch 19: train loss: 0.020167819589162007\n",
            "test loss: 0.019938460030336377\n",
            "\n",
            "Epoch 20: train loss: 0.019795198477343873\n",
            "test loss: 0.019779405926822445\n",
            "\n",
            "Epoch 21: train loss: 0.019609639655597522\n",
            "test loss: 0.019226097782437907\n",
            "\n",
            "Epoch 22: train loss: 0.019520003592053147\n",
            "test loss: 0.01929887225603507\n",
            "\n",
            "Epoch 23: train loss: 0.019431573206773266\n",
            "test loss: 0.01951143010130968\n",
            "\n",
            "Epoch 24: train loss: 0.019290698740718183\n",
            "test loss: 0.019078932857165436\n",
            "\n",
            "Epoch 25: train loss: 0.019198790811898132\n",
            "test loss: 0.019825699238738396\n",
            "\n",
            "Epoch 26: train loss: 0.019122126387174355\n",
            "test loss: 0.019548790505729437\n",
            "\n",
            "Epoch 27: train loss: 0.018945109760202646\n",
            "test loss: 0.019078385019823133\n",
            "\n",
            "Epoch 28: train loss: 0.01915220365884823\n",
            "test loss: 0.019717056331115864\n",
            "\n",
            "Epoch 29: train loss: 0.0189015601989153\n",
            "test loss: 0.019441977430939568\n",
            "\n",
            "Epoch 30: train loss: 0.018829706190645758\n",
            "test loss: 0.019257669992648826\n",
            "\n",
            "Epoch 31: train loss: 0.018956754743861482\n",
            "test loss: 0.01965782113969285\n",
            "\n",
            "Epoch 32: train loss: 0.018462524862541727\n",
            "test loss: 0.019069533146463914\n",
            "\n",
            "Epoch 33: train loss: 0.018668370656793196\n",
            "test loss: 0.01913813660648405\n",
            "\n",
            "Epoch 34: train loss: 0.018545270848821682\n",
            "test loss: 0.020353760063861887\n",
            "\n",
            "Epoch 35: train loss: 0.018361727839232123\n",
            "test loss: 0.018761695151457012\n",
            "\n",
            "Epoch 36: train loss: 0.01848513220231289\n",
            "test loss: 0.02143582784183299\n",
            "\n",
            "Epoch 37: train loss: 0.018229767303895222\n",
            "test loss: 0.01907617697431159\n",
            "\n",
            "Epoch 38: train loss: 0.01827195404262701\n",
            "test loss: 0.01935312366222965\n",
            "\n",
            "Epoch 39: train loss: 0.017931321349179286\n",
            "test loss: 0.018888660441801916\n",
            "\n",
            "Epoch 40: train loss: 0.017982205851326965\n",
            "test loss: 0.019799777296201544\n",
            "\n",
            "Epoch 41: train loss: 0.01815191367672106\n",
            "test loss: 0.01877681614841824\n",
            "\n",
            "Epoch 42: train loss: 0.017831453912515727\n",
            "test loss: 0.018801403441428055\n",
            "\n",
            "Epoch 43: train loss: 0.01789577419040627\n",
            "test loss: 0.018858346624034596\n",
            "\n",
            "Epoch 44: train loss: 0.017676574996602565\n",
            "test loss: 0.01931135151031786\n",
            "\n",
            "Epoch 45: train loss: 0.01764077641396428\n",
            "test loss: 0.019221714956997194\n",
            "\n",
            "Epoch 46: train loss: 0.017635688530064083\n",
            "test loss: 0.01939078892959291\n",
            "\n",
            "Epoch 47: train loss: 0.01769008582655507\n",
            "test loss: 0.018905086456501546\n",
            "\n",
            "Epoch 48: train loss: 0.017463369147128094\n",
            "test loss: 0.019304276286071304\n",
            "\n",
            "Epoch 49: train loss: 0.017257000186186387\n",
            "test loss: 0.021661384812283763\n",
            "\n",
            "Epoch 50: train loss: 0.017443513717802752\n",
            "test loss: 0.018891750604831014\n",
            "\n",
            "Epoch 51: train loss: 0.017433907666760225\n",
            "test loss: 0.0188809099910859\n",
            "\n",
            "Epoch 52: train loss: 0.01730555268784331\n",
            "test loss: 0.020306955376336082\n",
            "\n",
            "Epoch 53: train loss: 0.01709191381269366\n",
            "test loss: 0.019342431007822083\n",
            "\n",
            "Epoch 54: train loss: 0.017176942953877117\n",
            "test loss: 0.01880719979451719\n",
            "\n",
            "Epoch 55: train loss: 0.01708236274158371\n",
            "test loss: 0.01909088509398309\n",
            "\n",
            "Epoch 56: train loss: 0.017065197820544817\n",
            "test loss: 0.019404272944870907\n",
            "\n",
            "Epoch 57: train loss: 0.01722299860906455\n",
            "test loss: 0.019037709866548683\n",
            "\n",
            "Epoch 58: train loss: 0.01680060623439169\n",
            "test loss: 0.019233257852432867\n",
            "\n",
            "Epoch 59: train loss: 0.01667314796946893\n",
            "test loss: 0.020605814133998993\n",
            "\n",
            "Epoch 60: train loss: 0.016683011225931573\n",
            "test loss: 0.019108420339874047\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-89de7433489f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Computing the gradients over the backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Performing an optimization step from the current gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epochs = 100\n",
        "#Setting the model in training mode\n",
        "model.train()\n",
        "\n",
        "#Training loop\n",
        "start_time=time.time()\n",
        "epochs_train_losses = []\n",
        "epochs_test_losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_loss=[]\n",
        "    for x_batch, y_batch in training_generator:\n",
        "        # Removing previously computed gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Performing the forward pass on the current batch\n",
        "        y_pred = model(x_batch)\n",
        "        # Computing the loss given the current predictions\n",
        "        loss = criterion(y_pred.squeeze(), y_batch)\n",
        "        # Computing the gradients over the backward pass\n",
        "        loss.backward()\n",
        "        # Performing an optimization step from the current gradients\n",
        "        optimizer.step()\n",
        "        # Storing the current step's loss for display purposes\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    #showing last training loss after each epoch\n",
        "    epochs_train_losses.append(np.mean(train_loss))\n",
        "    print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
        "\n",
        "    #evaluating the model on the test set after each epoch\n",
        "    val_loss = evaluate_model(model,testing_generator,criterion)\n",
        "    epochs_test_losses.append(val_loss)\n",
        "    print('test loss: {}'.format(val_loss))\n",
        "    print(\"\")\n",
        "\n",
        "training_execution_time=time.time()-start_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ufKA82QtUNG"
      },
      "source": [
        "After training the model, a good practice is to analyze the training logs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPp7UMWFtUNG"
      },
      "outputs": [],
      "source": [
        "ma_window = 10\n",
        "\n",
        "plt.plot(np.arange(len(epochs_train_losses)-ma_window + 1)+1, np.convolve(epochs_train_losses, np.ones(ma_window)/ma_window, mode='valid'))\n",
        "plt.plot(np.arange(len(epochs_test_losses)-ma_window + 1)+1, np.convolve(epochs_test_losses, np.ones(ma_window)/ma_window, mode='valid'))\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train','valid'])\n",
        "#plt.ylim([0.01,0.06])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKIG4lGltUNG"
      },
      "outputs": [],
      "source": [
        "print(training_execution_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmlfXu8JtUNG"
      },
      "source": [
        "One can note here how the training loss decreases epoch after epoch. This means that the optimization is going well: the chosen learning rate allows to update the model towards a better solution (lower loss) for the **training dataset**. However, neural networks are known to be very expressive models. In fact, the universal approximation theorem shows that, with enough neurons/layers, one can model any function with a neural network {cite}`cybenko1989approximation`. Therefore, it is expected for a complex neural network to be able to fit almost perfectly the training data. But the ultimate goal is to obtain a model that generalizes well on unseen data (like the validation set). Looking at the validation loss, one can see that it starts by decreasing (with oscillations) and it reaches an optimum around 0.019, and stops decreasing (or even starts increasing). This phenomenon is referred to as [overfitting](Model_Selection).\n",
        "\n",
        "Many aspects can be improved in the training. Although one cannot measure the performance on the final test set while training, one can rely on a validation set and try to stop training before overfitting (see [](Model_Selection)). One can also change the optimization algorithm and parameters to speed up training and reach a better optimum. This is investigated later, in the optimization paragraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMXfZUoStUNH"
      },
      "source": [
        "For now, let us consider this final fitted model and evaluate it the same way as the other models in previous chapters. For this, let us create a prediction DataFrame and call the `performance_assessment` function from the shared functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H8OSl-ltUNH"
      },
      "outputs": [],
      "source": [
        "start_time=time.time()\n",
        "predictions_test = model(x_test.to(DEVICE))\n",
        "prediction_execution_time = time.time()-start_time\n",
        "predictions_train = model(x_train.to(DEVICE))\n",
        "print(\"Predictions took\", prediction_execution_time,\"seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et_SPn8ftUNH"
      },
      "outputs": [],
      "source": [
        "predictions_df=test_df\n",
        "predictions_df['predictions']=predictions_test.detach().cpu().numpy()\n",
        "\n",
        "performance_assessment(predictions_df, top_k_list=[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7p7Hl2ftUNH"
      },
      "source": [
        "This first shot feed-forward network already obtains a decent performance on the test set (refer to [Chapter 3.4](Baseline_FDS_Performances_Simulation) for comparison). But several elements can be modified to improve the AUC ROC, reduce the training time, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmI-G3VOtUNH"
      },
      "source": [
        "As stated above, for the first model, optimization was not carried out properly because the validation performance is not exploited during the training process. To avoid overfitting in practice, it is necessary to take it into account (See Chapter 5, [](Hold_Out_Validation))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVzwB5yHtUNH"
      },
      "outputs": [],
      "source": [
        "delta_valid = delta_test\n",
        "\n",
        "start_date_training_with_valid = start_date_training+datetime.timedelta(days=-(delta_delay+delta_valid))\n",
        "\n",
        "(train_df, valid_df)=get_train_test_set(transactions_df,start_date_training_with_valid,\n",
        "                                       delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)\n",
        "\n",
        "# By default, scales input data\n",
        "(train_df, valid_df)=scaleData(train_df, valid_df, input_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlX3-jJrtUNH"
      },
      "source": [
        "Let us implement an early stopping strategy. The idea is to detect overfitting, i.e. when validation error starts increasing, and stop the training process. Sometimes, the validation error might increase at a given epoch, but then decrease again. For that reason, it is important to also consider a patience parameter, i.e. a number of iterations for which the training process waits in order to make sure that the error is definitely increasing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d78rdKtQtUNH"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "\n",
        "    def __init__(self, patience=2, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = np.Inf\n",
        "\n",
        "    def continue_training(self,current_score):\n",
        "        if self.best_score > current_score:\n",
        "            self.best_score = current_score\n",
        "            self.counter = 0\n",
        "            if self.verbose:\n",
        "                print(\"New best score:\", current_score)\n",
        "        else:\n",
        "            self.counter+=1\n",
        "            if self.verbose:\n",
        "                print(self.counter, \" iterations since best score.\")\n",
        "\n",
        "        return self.counter <= self.patience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJzre91stUNI"
      },
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "\n",
        "model = SimpleFraudMLP(len(input_features), 1000).to(DEVICE)\n",
        "\n",
        "def prepare_generators(train_df,valid_df,batch_size=64):\n",
        "    x_train = torch.FloatTensor(train_df[input_features].values)\n",
        "    x_valid = torch.FloatTensor(valid_df[input_features].values)\n",
        "    y_train = torch.FloatTensor(train_df[output_feature].values)\n",
        "    y_valid = torch.FloatTensor(valid_df[output_feature].values)\n",
        "    train_loader_params = {'batch_size': batch_size,\n",
        "              'shuffle': True,\n",
        "              'num_workers': 0}\n",
        "    valid_loader_params = {'batch_size': batch_size,\n",
        "              'num_workers': 0}\n",
        "    # Generators\n",
        "\n",
        "    training_set = FraudDataset(x_train, y_train)\n",
        "    valid_set = FraudDataset(x_valid, y_valid)\n",
        "\n",
        "    training_generator = torch.utils.data.DataLoader(training_set, **train_loader_params)\n",
        "    valid_generator = torch.utils.data.DataLoader(valid_set, **valid_loader_params)\n",
        "\n",
        "    return training_generator,valid_generator\n",
        "\n",
        "training_generator,valid_generator = prepare_generators(train_df,valid_df,batch_size=64)\n",
        "\n",
        "criterion = torch.nn.BCELoss().to(DEVICE)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9383DKftUNI"
      },
      "source": [
        "The training loop can now be adapted to integrate early stopping:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "output_scroll"
        ],
        "id": "OyOTPxXKtUNI"
      },
      "outputs": [],
      "source": [
        "def training_loop(model,training_generator,valid_generator,optimizer,criterion,max_epochs=100,apply_early_stopping=True,patience=2,verbose=False):\n",
        "    #Setting the model in training mode\n",
        "    model.train()\n",
        "\n",
        "    if apply_early_stopping:\n",
        "        early_stopping = EarlyStopping(verbose=verbose,patience=patience)\n",
        "\n",
        "    all_train_losses = []\n",
        "    all_valid_losses = []\n",
        "\n",
        "    #Training loop\n",
        "    start_time=time.time()\n",
        "    for epoch in range(max_epochs):\n",
        "        model.train()\n",
        "        train_loss=[]\n",
        "        for x_batch, y_batch in training_generator:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(x_batch)\n",
        "            loss = criterion(y_pred.squeeze(), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "        #showing last training loss after each epoch\n",
        "        all_train_losses.append(np.mean(train_loss))\n",
        "        if verbose:\n",
        "            print('')\n",
        "            print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
        "        #evaluating the model on the test set after each epoch\n",
        "        valid_loss = evaluate_model(model,valid_generator,criterion)\n",
        "        all_valid_losses.append(valid_loss)\n",
        "        if verbose:\n",
        "            print('valid loss: {}'.format(valid_loss))\n",
        "        if apply_early_stopping:\n",
        "            if not early_stopping.continue_training(valid_loss):\n",
        "                if verbose:\n",
        "                    print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    training_execution_time=time.time()-start_time\n",
        "    return model,training_execution_time,all_train_losses,all_valid_losses\n",
        "\n",
        "model,training_execution_time,train_losses,valid_losses = training_loop(model,training_generator,valid_generator,optimizer,criterion,max_epochs=500,verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGyR0KBrtUNI"
      },
      "source": [
        "After 251 epochs, the model stops learning because the validation performance has not improved for three iterations. Here the optimal model (from epoch 248) is not saved, but this could be implemented by simply adding `torch.save(model.state_dict(), checkpoint_path)` in the `EarlyStopping` class whenever a new best performance is reached. This allows reloading the saved best checkpoint at the end of the training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtmaMqgbtUNI"
      },
      "source": [
        "Now that a clean optimization process is defined, one can consider several solutions to speed up and improve convergence towards a decent extremum. The most natural way to do so is to play with the optimizer hyperparameters like the learning rate and the batch size. With a large learning rate, gradient descent is fast at the beginning, but then the optimizer struggles to find the minimum. Adaptive learning rate techniques like Adam/RMSProp take into account the steepness by normalizing the learning rate with respect to the gradient norm. Below are the formulas to update on a model parameter $w_t$ with Adam.\n",
        "\n",
        "$w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\hat{v_t}+\\epsilon}}*\\hat{m_t}$\n",
        "\n",
        "Where:\n",
        "\n",
        "$m_t = \\beta_1 * m_{t-1} + (1-\\beta_1)*\\nabla w_t$\n",
        "\n",
        "$v_t = \\beta_2 * v_{t-1} + (1-\\beta_2)*(\\nabla w_t)^2$\n",
        "\n",
        "$\\hat{m_t}=\\frac{m_t}{1-\\beta_1^t}$\n",
        "\n",
        "$\\hat{v_t}=\\frac{v_t}{1-\\beta_2^t}$\n",
        "\n",
        "The difference with SGD is that here the learning rate is normalized using the \"gradient norm\" ($\\approx \\hat{m_t}$). To be more precise, the approach does not use the \"raw\" gradient $\\nabla w_t$ and gradient norm $\\nabla w_t^2$ but a momentum instead (convex combination between previous values and the current value), respectively $m_t$ and $v_t$. It also applies a decay over the iterations.\n",
        "\n",
        "Let us try Adam with an initial learning rate of `0.0005` to see the difference with regular SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "output_scroll"
        ],
        "id": "jmuooZZMtUNI"
      },
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "model = SimpleFraudMLP(len(input_features), 1000).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
        "model,training_execution_time,train_losses_adam,valid_losses_adam = training_loop(model,training_generator,valid_generator,optimizer,criterion,verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvyDL8LKtUNI"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.arange(len(train_losses_adam))+1, train_losses_adam)\n",
        "plt.plot(np.arange(len(valid_losses_adam))+1, valid_losses_adam)\n",
        "plt.title(\"ADAM\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train','valid'])\n",
        "plt.ylim([0.01,0.06])\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(np.arange(len(train_losses))+1, train_losses)\n",
        "plt.plot(np.arange(len(valid_losses))+1, valid_losses)\n",
        "plt.title(\"SGD\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train','valid'])\n",
        "plt.ylim([0.01,0.06])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA2EIU-4tUNJ"
      },
      "source": [
        "The optimization is much faster with Adam (10 times fewer epochs) and it reaches a better optimum. Of course, increasing patience or changing the learning rate with SGD would probably help improve both speed and optimum. Nevertheless, Adam will be retained for the rest of the chapter as it usually allows very decent performance **without significant tuning**. To build the final neural network later, the tuning will mostly be made using batch size and initial learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twxpqaVYtUNJ"
      },
      "source": [
        "Here are only mentioned the optimizer's choice and some hyperparameters tuning for the optimization process. But keep in mind that neural network optimization is a very wide and active area of research/engineering. For instance, with Deep Networks, one can apply batch normalization after each layer to standardize the distribution and speed up convergence. One can also reduce the learning rate when validation loss reaches a plateau (`torch.optim.ReduceLROnPlateau`). For a full guide on Deep Learning optimization, we recommend {cite}`ruder2016overview,le2011optimization`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dOr-LFAtUNJ"
      },
      "source": [
        "## Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzzpSVZftUNJ"
      },
      "source": [
        "A classical way to improve generalization and reach a better validation performance is to regularize the model. Roughly speaking, regularization consists in limiting the model expressiveness in order to reduce overfitting.\n",
        "\n",
        "The most common technique to regularize a machine learning model is to restrict its parameter space, for instance, its norm, by adding a term in the optimization problem. Additionally, to minimize the discrepancy between ground truth and prediction, integrating an L1 norm (resp. L2 norm) term in the loss will entail parameter sparsity (resp. will limit parameters amplitude). The initial solution space is generally full of equivalent solutions (e.g. with linear activations, dividing all input weights of a neuron by 2 and multiplying all of its output weights by 2 leads to an equivalent model), so the restrictions entailed by regularization not only limits overfitting but also reduces the search and can help with optimization. Finally, selecting a solution with minimal norm follows the principle of \"All things being equal, the simplest solution tends to be the best one\", a scientific principle often referred to as the [Occam's razor](https://en.wikipedia.org/wiki/Occam's_razor).\n",
        "\n",
        "In contrast to adding loss terms, there is a regularization technique specifically designed for Neural Networks called dropout. Dropout consists in randomly dropping some neurons from the network at each training step. More precisely, one fixes a dropout parameter p∈[0,1], and, for each mini-batch, for each neuron, performs a coin toss (Bernoulli) with probability p. If positive, one temporarily sets the neuron's weights to zero (so that the dropped neuron is not considered during the forward and backward passes). It is equivalent to training a random sub-network at each mini-batch (Figure 5), and it can be proven that this has an L2-regularization effect on specific architectures {cite}`srivastava2014dropout`.\n",
        "\n",
        "![alt text](https://github.com/Fraud-Detection-Handbook/fraud-detection-handbook/blob/main/Chapter_7_DeepLearning/images/dropout.png?raw=1)\n",
        "Image source: {cite}`srivastava2014dropout`\n",
        "\n",
        "To implement it, let us define a new model with an additional `torch.nn.Dropout` layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLLot7UItUNJ"
      },
      "outputs": [],
      "source": [
        "class SimpleFraudMLPWithDropout(torch.nn.Module):\n",
        "\n",
        "        def __init__(self, input_size, hidden_size,p):\n",
        "            super(SimpleFraudMLPWithDropout, self).__init__()\n",
        "            # parameters\n",
        "            self.input_size = input_size\n",
        "            self.hidden_size  = hidden_size\n",
        "            self.p = p\n",
        "\n",
        "            #input to hidden\n",
        "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "            self.relu = torch.nn.ReLU()\n",
        "            #hidden to output\n",
        "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
        "            self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "            self.dropout = torch.nn.Dropout(self.p)\n",
        "\n",
        "        def forward(self, x):\n",
        "\n",
        "            hidden = self.fc1(x)\n",
        "            hidden = self.relu(hidden)\n",
        "\n",
        "            hidden = self.dropout(hidden)\n",
        "\n",
        "            output = self.fc2(hidden)\n",
        "            output = self.sigmoid(output)\n",
        "\n",
        "            return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKS-DjlXtUNJ"
      },
      "source": [
        "Note that setting the model in training/evaluation mode with the methods `model.eval()` and `model.train()` take all its significance here. In particular, the dropout layer in the forward pass is only applied when the model is in training mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyHNDoNYtUNJ"
      },
      "outputs": [],
      "source": [
        "seed_everything(SEED)\n",
        "model = SimpleFraudMLPWithDropout(len(input_features), 1000,0.2).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0005)\n",
        "model,training_execution_time,train_losses_dropout,valid_losses_dropout = training_loop(model,training_generator,valid_generator,optimizer,criterion,verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_bLDJKItUNJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(train_losses_adam))+1, train_losses_adam)\n",
        "plt.plot(np.arange(len(valid_losses_adam))+1, valid_losses_adam)\n",
        "plt.plot(np.arange(len(train_losses_dropout))+1, train_losses_dropout)\n",
        "plt.plot(np.arange(len(valid_losses_dropout))+1, valid_losses_dropout)\n",
        "plt.title(\"Dropout effect\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train w/o dropout','valid w/o dropout','train w/ dropout','valid w/ dropout'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q6daUMftUNK"
      },
      "source": [
        "It is generally reported that a small dropout value can lead to better generalization results than no dropout, but it can sometimes be the opposite if the training data are very rich, if the training distribution is close to the valid/test distribution, and if the model is already not too expressive. So, the best practice is to consider dropout as a hyperparameter (that could be set to 0) and tune it with a hyperparameter search.\n",
        "\n",
        "In addition to the L2-regularization effect, dropout can be seen as a very powerful mechanism that mimics ensembling strategies like bagging (the model can be seen as an ensemble of submodels trained on different subsets of data) {cite}`goodfellow2016deep`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da7Rvka4tUNK"
      },
      "source": [
        "## Scaling the inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0srFI-DtUNK"
      },
      "source": [
        "XGBoost and random forests learn splits on single features and therefore are robust to the scale and distribution of the values. On the contrary, in a neural network, each neuron of the first layer learns a linear combination of all the features. Therefore, it is easier to train the neurons when all features have the same range and are normally distributed. The first property can be easily implemented by applying min-max or standard scaling on the features. As for the second property, it depends on the original distribution of the features. Some of them are not normally distributed and have non-linear scales (e.g. amount): increasing the amount by 5 dollars should not have the same effect if the starting point is 5 dollars or if the starting point is 100 dollars. It turns out that applying the log function on such features can make their distribution slightly more normal which makes it easier for feed-forward neural networks to learn from them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRFG7b5-tUNK"
      },
      "outputs": [],
      "source": [
        "(train_df, valid_df)=get_train_test_set(transactions_df,start_date_training_with_valid,\n",
        "                                       delta_train=delta_train,delta_delay=delta_delay,delta_test=delta_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHtPtzmutUNK"
      },
      "source": [
        "For instance here is how the original amounts look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pqMQYc1tUNK"
      },
      "outputs": [],
      "source": [
        "_ = plt.hist(train_df['TX_AMOUNT'].values,bins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIPNf3natUNK"
      },
      "source": [
        "And now let us apply the log function to it. To obtain a positive log for the feature that is in [0,+∞[, an idea is to add 1 and then apply the log function (which is equivalent to applying the `log1p` function in `numpy`). This leads to a preprocessed feature that belongs to [0,+∞[ and it can then be standardized. Here is how the amounts are distributed after all these steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyndO6vDtUNK"
      },
      "outputs": [],
      "source": [
        "_ = plt.hist(sklearn.preprocessing.StandardScaler().fit_transform(np.log1p(train_df['TX_AMOUNT'].values).reshape(-1, 1)),bins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hu9FY7jtUNL"
      },
      "source": [
        "Note that here, our artificial data were generated with Gaussians so the `np.log1p` is not very useful in practice. But keep in mind that on real-world data, the original scale of features like amount is far from normally distributed and this operation turns out to be quite often useful.\n",
        "\n",
        "Let us forget about the log for now and just analyze the impact of scaling the features on our Neural Network's training. More precisely, let us see the difference between no scaling at all and standard scaling. A smaller learning rate of `0.0001` will be chosen here for the experiment without scaling to avoid divergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6JJGAHYtUNL"
      },
      "outputs": [],
      "source": [
        "#we did not call the function scaleData this time\n",
        "seed_everything(SEED)\n",
        "training_generator,valid_generator = prepare_generators(train_df,valid_df,batch_size=64)\n",
        "\n",
        "model = SimpleFraudMLPWithDropout(len(input_features), 1000,0.2).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "model,training_execution_time,train_losses_without_scaling,valid_losses_without_scaling = training_loop(model,training_generator,valid_generator,optimizer,criterion,verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2X9tWWjtUNL"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(train_losses_without_scaling))+1, train_losses_without_scaling)\n",
        "plt.plot(np.arange(len(valid_losses_without_scaling))+1, valid_losses_without_scaling)\n",
        "plt.plot(np.arange(len(train_losses_dropout))+1, train_losses_dropout)\n",
        "plt.plot(np.arange(len(valid_losses_dropout))+1, valid_losses_dropout)\n",
        "plt.title('Scaling effect')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['train w/o scaling','valid w/o scaling','train w/ scaling','valid w/ scaling'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zreLaEURtUNL"
      },
      "source": [
        "The train/valid losses are smoother and reach much better levels faster when the data is normalized with standard scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TGogoaetUNL"
      },
      "outputs": [],
      "source": [
        "# Let us rescale data for the next parts\n",
        "(train_df, valid_df)=scaleData(train_df, valid_df,input_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB32zSb7tUNU"
      },
      "source": [
        "## Benchmark summary\n",
        "\n",
        "Let us finally retrieve the performance results obtained in [Chapter 5](Model_Selection_Comparison_Performances) with decision tree, logistic regression, random forest and XGBoost, and compare them with those obtained with a feed-forward neural network. The results can be retrieved by loading the `performances_model_selection.pkl` and `performances_model_selection_nn.pkl` pickle files, and summarized with the `get_summary_performances` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "EsuiCKmqtUNU"
      },
      "outputs": [],
      "source": [
        "# Load performance results for decision tree, logistic regression, random forest and XGBoost\n",
        "filehandler = open('../Chapter_5_ModelValidationAndSelection/performances_model_selection.pkl', 'rb')\n",
        "(performances_df_dictionary, execution_times) = pickle.load(filehandler)\n",
        "\n",
        "# Load performance results for feed-forward neural network\n",
        "filehandler = open('performances_model_selection_nn.pkl', 'rb')\n",
        "(performances_df_dictionary_nn, execution_times_nn) = pickle.load(filehandler)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('So far so good')"
      ],
      "metadata": {
        "id": "VDO7Qhbo7CLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "hide-cell"
        ],
        "id": "K7n-0y1-tUNU"
      },
      "outputs": [],
      "source": [
        "performances_df_dt=performances_df_dictionary['Decision Tree']\n",
        "summary_performances_dt=get_summary_performances(performances_df_dt, parameter_column_name=\"Parameters summary\")\n",
        "\n",
        "performances_df_lr=performances_df_dictionary['Logistic Regression']\n",
        "summary_performances_lr=get_summary_performances(performances_df_lr, parameter_column_name=\"Parameters summary\")\n",
        "\n",
        "performances_df_rf=performances_df_dictionary['Random Forest']\n",
        "summary_performances_rf=get_summary_performances(performances_df_rf, parameter_column_name=\"Parameters summary\")\n",
        "\n",
        "performances_df_xgboost=performances_df_dictionary['XGBoost']\n",
        "summary_performances_xgboost=get_summary_performances(performances_df_xgboost, parameter_column_name=\"Parameters summary\")\n",
        "\n",
        "performances_df_nn=performances_df_dictionary_nn['Neural Network']\n",
        "summary_performances_nn=get_summary_performances(performances_df_nn, parameter_column_name=\"Parameters summary\")\n",
        "\n",
        "summary_test_performances = pd.concat([summary_performances_dt.iloc[2,:],\n",
        "                                       summary_performances_lr.iloc[2,:],\n",
        "                                       summary_performances_rf.iloc[2,:],\n",
        "                                       summary_performances_xgboost.iloc[2,:],\n",
        "                                       summary_performances_nn.iloc[2,:],\n",
        "                                      ],axis=1)\n",
        "\n",
        "summary_test_performances.columns=['Decision Tree', 'Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_fh5qbOtUNU"
      },
      "source": [
        "The results are summarized in a `summary_test_performances` table. Rows provide the average performance results on the test sets in terms of AUC ROC, Average Precision and CP@100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYgIgpbbtUNU"
      },
      "outputs": [],
      "source": [
        "summary_test_performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUHVyp3mtUNU"
      },
      "source": [
        "Overall, it appears that our simple feed-forward neural network is a good competitor in terms of predictive performance for the fraud detection problem, providing the best performances in terms of AUC ROC and CP@100, and competitive performances in terms of Average Precision. Moreover, it benefits from many advantages (e.g. its ability for incremental learning), as mentioned in the previous section."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Adversarial Attacks"
      ],
      "metadata": {
        "id": "bLt8bTmJtxO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3vGMQc9luTDI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPH841z0v_08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IlTwADzbuTFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# White Box Attacks\n"
      ],
      "metadata": {
        "id": "CxCCsfg8tyHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7HNN8vkluRnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sA8sz_t3uRqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adversarial-robustness-toolbox\n"
      ],
      "metadata": {
        "id": "egKl4kxQuPWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from art.att"
      ],
      "metadata": {
        "id": "s8fqFee3uPwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pku3nO6euP7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Black Box Attacks\n"
      ],
      "metadata": {
        "id": "xAqdTYxQuBPc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsT9zt7xuQj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_dN2dah7uQmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ShGqpQ95uQpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Attack\n"
      ],
      "metadata": {
        "id": "XcSVMWyMuFR5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zo74m73suULR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hIBmNfl8uUNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUCT_F6DuUQG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}